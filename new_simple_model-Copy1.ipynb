{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240acf99",
   "metadata": {},
   "source": [
    "Dataset organized kindly by user Mourad. https://www.kaggle.com/msheriey/104-flowers-garden-of-eden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57708f20",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12d75b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.applications import Xception, DenseNet201\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import random\n",
    "from PIL import Image\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfd567",
   "metadata": {},
   "source": [
    "# global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b60659f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image size options: 192, 224, 311, 512\n",
    "IMAGE_DIMENSION = 192\n",
    "VECTOR_LEN = IMAGE_DIMENSION**2\n",
    "NUM_CLASS = 104\n",
    "\n",
    "TRAIN_DIR = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/train'\n",
    "VAL_DIR = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/val'\n",
    "TEST_DIR = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/test'\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989fb6c",
   "metadata": {},
   "source": [
    "# generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68dae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img, train_lb = next(generate_train_dataset()) # x_train, y_train\n",
    "# val_img, val_lb = next(generate_val_dataset()) # x_test, y_test\n",
    "# test_img, test_lb = next(generate_test_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4640b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58786387",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img, train_lb = next(train_generator)\n",
    "val_img, val_lb = next(val_generator)\n",
    "test_img, test_lb = next(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d44fe8",
   "metadata": {},
   "source": [
    "# explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d29d5271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 192, 192, 3), (10, 104))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape, train_lb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6da51c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lb[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c474f7",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ad7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img.shape, train_lb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa81543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(train_img))\n",
    "print(np.shape(train_lb))\n",
    "print(np.shape(val_img))\n",
    "print(np.shape(val_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_unrow = train_img.reshape(TRAIN_BATCH_SIZE, -1).T\n",
    "np.shape(train_img_unrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7a025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_img_unrow = val_img.reshape(VAL_BATCH_SIZE, -1).T\n",
    "np.shape(val_img_unrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14efe9",
   "metadata": {},
   "source": [
    "# base model cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2),\n",
    "                     strides=2))\n",
    "model.add(layers.Flatten())        \n",
    "# possibly more layers here\n",
    "\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit_generator(train_generator, epochs=5, validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator, epochs=5, validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82d656",
   "metadata": {},
   "source": [
    "### High train accuracy and low val accuracy. Adding more layers and normalizing to bring them closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ef8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with multiple hidden layers\n",
    "model_2 = models.Sequential()\n",
    "\n",
    "model_2.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model_2.add(layers.MaxPooling2D(pool_size=(2,2),\n",
    "                     strides=2))\n",
    "model_2.add(layers.Flatten())        \n",
    "# possibly more layers here\n",
    "model_2.add(layers.BatchNormalization())\n",
    "model_2.add(layers.Dense(128, activation='relu'))\n",
    "model_2.add(layers.Dropout(0.5, input_shape=(IMAGE_DIMENSION,)))\n",
    "model_2.add(layers.Dense(64, activation='relu'))\n",
    "model_2.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model_2.add(layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf614d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75cb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimizer, cost lost function, and scoring metric\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f554a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2.fit(train_generator, epochs=5, validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac895335",
   "metadata": {},
   "source": [
    "### introduce transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xception_model = Xception(weights='imagenet', include_top=False, input_shape=[IMAGE_DIMENSION, IMAGE_DIMENSION, 3])\n",
    "Xception_model.trainable = False\n",
    "\n",
    "model_tl_1 = Sequential([\n",
    "    Xception_model,\n",
    "    layers.Conv2D(filters=32, \n",
    "        kernel_size=(2,2),\n",
    "        strides=(1,1),\n",
    "        activation='relu',\n",
    "        padding = 'same',\n",
    "        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "        data_format = 'channels_last'),\n",
    "    layers.Flatten(),\n",
    "    Dense(NUM_CLASS, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_tl_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl_1.fit(train_generator, epochs=5, validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912b156",
   "metadata": {},
   "source": [
    "### using globalaveragepooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e28a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xception_model = Xception(weights='imagenet', include_top=False, input_shape=[IMAGE_DIMENSION, IMAGE_DIMENSION, 3])\n",
    "Xception_model.trainable = False\n",
    "\n",
    "model_tl_2 = Sequential([\n",
    "    Xception_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    layers.Flatten(),\n",
    "    model_4.add(layers.Dropout(0.2, input_shape=(IMAGE_DIMENSION,)))\n",
    "    Dense(NUM_CLASS, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_tl_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl_2.fit(train_generator, epochs=50, validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9a6c0",
   "metadata": {},
   "source": [
    "### even more basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab760f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=5, validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098f873",
   "metadata": {},
   "source": [
    "### overfitting, going to introduce some data augmentation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, horizontal_flip=True, rotation_range=15).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=5, validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714f828",
   "metadata": {},
   "source": [
    "### attempt to reduce overfitting (dropout) and more epochs (25) to run overnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "076014f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=15,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5]\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52d22a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 95, 95, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 47, 47, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 46, 46, 64)        16448     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 135424)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 135424)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8667200   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 104)               6760      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 104)               0         \n",
      "=================================================================\n",
      "Total params: 8,699,080\n",
      "Trainable params: 8,699,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (2,2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (2,2), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bb8fa4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cf9c2160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2cf9c2160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      " 812/1145 [====================>.........] - ETA: 26s - loss: 4.1548 - accuracy: 0.0811"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_13678/3257430881.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=25,\n\u001b[0m\u001b[1;32m      2\u001b[0m           validation_data=val_generator, use_multiprocessing=False)\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=25,\n",
    "          validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59981ae6",
   "metadata": {},
   "source": [
    "# much better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad87a1",
   "metadata": {},
   "source": [
    "### introduce learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a35592",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (2,2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (2,2), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=25,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4df9c9",
   "metadata": {},
   "source": [
    "### reintroduce transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a46ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1b948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fbcf2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xception (Functional)        (None, 6, 6, 2048)        20861480  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 104)               213096    \n",
      "=================================================================\n",
      "Total params: 21,074,576\n",
      "Trainable params: 213,096\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Xception_model = Xception(weights='imagenet', include_top=False, input_shape=[IMAGE_DIMENSION, IMAGE_DIMENSION, 3])\n",
    "Xception_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    Xception_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    layers.Flatten(),\n",
    "#    Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)),\n",
    "#    Dense(64, activation='relu'),\n",
    "    Dense(NUM_CLASS, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "508e3683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2f4ba71f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2f4ba71f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - ETA: 0s - loss: 3.5389 - accuracy: 0.2547WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x17fbcd1f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x17fbcd1f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - 386s 336ms/step - loss: 3.5385 - accuracy: 0.2547 - val_loss: 385.8642 - val_accuracy: 0.0116\n",
      "Epoch 2/20\n",
      "1145/1145 [==============================] - 386s 337ms/step - loss: 2.1638 - accuracy: 0.4960 - val_loss: 712.2999 - val_accuracy: 0.0221\n",
      "Epoch 3/20\n",
      "  40/1145 [>.............................] - ETA: 4:48 - loss: 1.9368 - accuracy: 0.5162"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_13086/483433994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=20, \n\u001b[0m\u001b[1;32m      2\u001b[0m           validation_data=val_generator, use_multiprocessing=False)\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=20, \n",
    "          validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f7e1e",
   "metadata": {},
   "source": [
    "### attempt to remove flattening to see how that affects the model. also introduce dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28dbc1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1595525d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29191ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xception (Functional)        (None, 6, 6, 2048)        20861480  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 104)               213096    \n",
      "=================================================================\n",
      "Total params: 21,074,576\n",
      "Trainable params: 213,096\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Xception_model = Xception(weights='imagenet', include_top=False, input_shape=[IMAGE_DIMENSION, IMAGE_DIMENSION, 3])\n",
    "Xception_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    Xception_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "#    layers.Flatten(),\n",
    "    Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)),\n",
    "#    Dense(64, activation='relu'),\n",
    "    Dense(NUM_CLASS, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d5cf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x355b7c820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x355b7c820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - ETA: 0s - loss: 3.7230 - accuracy: 0.2144WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x355b96430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x355b96430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - 387s 337ms/step - loss: 3.7226 - accuracy: 0.2145 - val_loss: 78.8614 - val_accuracy: 0.0078\n",
      "Epoch 2/20\n",
      "1145/1145 [==============================] - 387s 338ms/step - loss: 2.4672 - accuracy: 0.4203 - val_loss: 194.3671 - val_accuracy: 0.0084\n",
      "Epoch 3/20\n",
      "1145/1145 [==============================] - 386s 337ms/step - loss: 2.1707 - accuracy: 0.4765 - val_loss: 361.2457 - val_accuracy: 0.0111\n",
      "Epoch 4/20\n",
      " 553/1145 [=============>................] - ETA: 2:34 - loss: 2.0949 - accuracy: 0.4915"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_13086/483433994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=20, \n\u001b[0m\u001b[1;32m      2\u001b[0m           validation_data=val_generator, use_multiprocessing=False)\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=20, \n",
    "          validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cce802",
   "metadata": {},
   "source": [
    "### shuffle for validation is turned off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ec510e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a767da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af60c7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xception (Functional)        (None, 6, 6, 2048)        20861480  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 104)               213096    \n",
      "=================================================================\n",
      "Total params: 21,074,576\n",
      "Trainable params: 213,096\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Xception_model = Xception(weights='imagenet', include_top=False, input_shape=[IMAGE_DIMENSION, IMAGE_DIMENSION, 3])\n",
    "Xception_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    Xception_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "#    layers.Flatten(),\n",
    "    Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)),\n",
    "#    Dense(64, activation='relu'),\n",
    "    Dense(NUM_CLASS, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "791c08e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2f2d6a430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2f2d6a430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - ETA: 0s - loss: 3.7176 - accuracy: 0.2157WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2c7eaff70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2c7eaff70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - 382s 333ms/step - loss: 3.7172 - accuracy: 0.2158 - val_loss: 394.1703 - val_accuracy: 0.0210\n",
      "Epoch 2/20\n",
      "1145/1145 [==============================] - 382s 334ms/step - loss: 2.4707 - accuracy: 0.4313 - val_loss: 118.3906 - val_accuracy: 0.0100\n",
      "Epoch 3/20\n",
      "1145/1145 [==============================] - 381s 332ms/step - loss: 2.1871 - accuracy: 0.4758 - val_loss: 249.7218 - val_accuracy: 0.0067\n",
      "Epoch 4/20\n",
      "1145/1145 [==============================] - 382s 333ms/step - loss: 2.0550 - accuracy: 0.5009 - val_loss: 164.3312 - val_accuracy: 0.0086\n",
      "Epoch 5/20\n",
      "1145/1145 [==============================] - 383s 334ms/step - loss: 1.9652 - accuracy: 0.5169 - val_loss: 560.9910 - val_accuracy: 0.0148\n",
      "Epoch 6/20\n",
      "1145/1145 [==============================] - 382s 334ms/step - loss: 1.9439 - accuracy: 0.5188 - val_loss: 888.7313 - val_accuracy: 0.0097\n",
      "Epoch 7/20\n",
      "1145/1145 [==============================] - 381s 333ms/step - loss: 1.9056 - accuracy: 0.5294 - val_loss: 370.9457 - val_accuracy: 0.0070\n",
      "Epoch 8/20\n",
      "1145/1145 [==============================] - 383s 335ms/step - loss: 1.8430 - accuracy: 0.5393 - val_loss: 215.6325 - val_accuracy: 0.0070\n",
      "Epoch 9/20\n",
      "1145/1145 [==============================] - 382s 333ms/step - loss: 1.9256 - accuracy: 0.5159 - val_loss: 405.2083 - val_accuracy: 0.0067\n",
      "Epoch 10/20\n",
      "1145/1145 [==============================] - 382s 334ms/step - loss: 1.8498 - accuracy: 0.5405 - val_loss: 282.9030 - val_accuracy: 0.0040\n",
      "Epoch 11/20\n",
      "1145/1145 [==============================] - 386s 337ms/step - loss: 1.8776 - accuracy: 0.5247 - val_loss: 573.9048 - val_accuracy: 0.0084\n",
      "Epoch 12/20\n",
      " 495/1145 [===========>..................] - ETA: 2:49 - loss: 1.8103 - accuracy: 0.5376"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_13086/483433994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=20, \n\u001b[0m\u001b[1;32m      2\u001b[0m           validation_data=val_generator, use_multiprocessing=False)\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=20, \n",
    "          validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45556089",
   "metadata": {},
   "source": [
    "### shuffle is turned back on. going to a model with densenet201 instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b41c6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09dae7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ca0f004",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_13086/2884166489.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDenseNet_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseNet201\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIMAGE_DIMENSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_DIMENSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mDenseNet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model = Sequential([\n\u001b[1;32m      5\u001b[0m     \u001b[0mDenseNet_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/applications/densenet.py\u001b[0m in \u001b[0;36mDenseNet201\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 classes=1000):\n\u001b[1;32m    355\u001b[0m   \u001b[0;34m\"\"\"Instantiates the Densenet201 architecture.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m   return DenseNet([6, 12, 48, 32], include_top, weights, input_tensor,\n\u001b[0m\u001b[1;32m    357\u001b[0m                   input_shape, pooling, classes)\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/applications/densenet.py\u001b[0m in \u001b[0;36mDenseNet\u001b[0;34m(blocks, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mcache_subdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             file_hash='c13680b51ded0fb44dff2d8f86ac8bb1')\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2232\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   2233\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2234\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    660\u001b[0m   \"\"\"\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m'keras_version'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m     \u001b[0moriginal_keras_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keras_version'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0moriginal_keras_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "DenseNet_model = DenseNet201(weights='imagenet', include_top=False, input_shape=[IMAGE_DIMENSION, IMAGE_DIMENSION, 3])\n",
    "DenseNet_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    DenseNet_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "#    layers.Flatten(),\n",
    "    Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)),\n",
    "#    Dense(64, activation='relu'),\n",
    "    Dense(NUM_CLASS, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac5d4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=20, \n",
    "          validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca00e9",
   "metadata": {},
   "source": [
    "### WEIRD ATTEMPT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36f2217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f512e5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84426eb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`load_weights` requires h5py when loading weights from HDF5.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_13678/566271307.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXception_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIMAGE_DIMENSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_DIMENSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mXception_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model = Sequential([\n\u001b[1;32m      5\u001b[0m     \u001b[0mXception_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/applications/xception.py\u001b[0m in \u001b[0;36mXception\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0mcache_subdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           file_hash='b0042744bf5b25fce3cb969f33bebb97')\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2217\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2219\u001b[0;31m       raise ImportError(\n\u001b[0m\u001b[1;32m   2220\u001b[0m           '`load_weights` requires h5py when loading weights from HDF5.')\n\u001b[1;32m   2221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `load_weights` requires h5py when loading weights from HDF5."
     ]
    }
   ],
   "source": [
    "Xception_model = Xception(weights='imagenet', include_top=False, input_shape=[IMAGE_DIMENSION, IMAGE_DIMENSION, 3])\n",
    "Xception_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    Xception_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    layers.Flatten(),\n",
    "#    Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)),\n",
    "#    Dense(64, activation='relu'),\n",
    "    Dense(NUM_CLASS, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d33d1503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2f4ba71f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2f4ba71f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - ETA: 0s - loss: 3.5389 - accuracy: 0.2547WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x17fbcd1f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x17fbcd1f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - 386s 336ms/step - loss: 3.5385 - accuracy: 0.2547 - val_loss: 385.8642 - val_accuracy: 0.0116\n",
      "Epoch 2/20\n",
      "1145/1145 [==============================] - 386s 337ms/step - loss: 2.1638 - accuracy: 0.4960 - val_loss: 712.2999 - val_accuracy: 0.0221\n",
      "Epoch 3/20\n",
      "  40/1145 [>.............................] - ETA: 4:48 - loss: 1.9368 - accuracy: 0.5162"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_13086/483433994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=20, \n\u001b[0m\u001b[1;32m      2\u001b[0m           validation_data=val_generator, use_multiprocessing=False)\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=20, \n",
    "          validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46895f32",
   "metadata": {},
   "source": [
    "### try a different pretrained model. ENet B7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd22f0a",
   "metadata": {},
   "source": [
    "### ensemble for more trained images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea398cd",
   "metadata": {},
   "source": [
    "### removal of transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0e1d442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59961d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 47, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 47, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 45, 46, 4)         388       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 45, 46, 4)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8280)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                529984    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 104)               6760      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 104)               0         \n",
      "=================================================================\n",
      "Total params: 542,300\n",
      "Trainable params: 542,236\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.70, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.Conv2D(4, (3,2), activation='relu'))\n",
    "model.add(layers.Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88f46d10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d82564c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d82564c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - ETA: 0s - loss: 4.2987 - accuracy: 0.0631WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x17258d160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x17258d160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - 75s 65ms/step - loss: 4.2986 - accuracy: 0.0631 - val_loss: 4.6266 - val_accuracy: 0.0361\n",
      "Epoch 2/25\n",
      "1145/1145 [==============================] - 75s 66ms/step - loss: 3.9596 - accuracy: 0.0891 - val_loss: 22.4009 - val_accuracy: 0.0402\n",
      "Epoch 3/25\n",
      "1145/1145 [==============================] - 75s 65ms/step - loss: 3.7988 - accuracy: 0.1024 - val_loss: 87.8962 - val_accuracy: 0.0555\n",
      "Epoch 4/25\n",
      "1145/1145 [==============================] - 76s 66ms/step - loss: 3.7347 - accuracy: 0.1094 - val_loss: 33.3124 - val_accuracy: 0.0523\n",
      "Epoch 5/25\n",
      "1145/1145 [==============================] - 75s 66ms/step - loss: 3.6525 - accuracy: 0.1110 - val_loss: 51.2424 - val_accuracy: 0.0544\n",
      "Epoch 6/25\n",
      "1145/1145 [==============================] - 75s 66ms/step - loss: 3.6823 - accuracy: 0.1132 - val_loss: 94.6410 - val_accuracy: 0.0601\n",
      "Epoch 7/25\n",
      "1145/1145 [==============================] - 74s 64ms/step - loss: 3.6455 - accuracy: 0.1259 - val_loss: 48.5730 - val_accuracy: 0.0466\n",
      "Epoch 8/25\n",
      "1145/1145 [==============================] - 72s 63ms/step - loss: 3.5996 - accuracy: 0.1248 - val_loss: 63.9997 - val_accuracy: 0.0596\n",
      "Epoch 9/25\n",
      "1145/1145 [==============================] - 74s 65ms/step - loss: 3.5840 - accuracy: 0.1238 - val_loss: 58.9102 - val_accuracy: 0.0515\n",
      "Epoch 10/25\n",
      "1145/1145 [==============================] - 76s 67ms/step - loss: 3.5454 - accuracy: 0.1332 - val_loss: 55.0750 - val_accuracy: 0.0636\n",
      "Epoch 11/25\n",
      "1145/1145 [==============================] - 77s 67ms/step - loss: 3.5893 - accuracy: 0.1288 - val_loss: 106.3863 - val_accuracy: 0.0628\n",
      "Epoch 12/25\n",
      "1145/1145 [==============================] - 75s 65ms/step - loss: 3.6079 - accuracy: 0.1269 - val_loss: 93.4040 - val_accuracy: 0.0749\n",
      "Epoch 13/25\n",
      "1145/1145 [==============================] - 75s 66ms/step - loss: 3.5762 - accuracy: 0.1292 - val_loss: 120.4898 - val_accuracy: 0.0682\n",
      "Epoch 14/25\n",
      "1145/1145 [==============================] - 75s 65ms/step - loss: 3.5805 - accuracy: 0.1329 - val_loss: 141.4276 - val_accuracy: 0.0402\n",
      "Epoch 15/25\n",
      "1145/1145 [==============================] - 73s 64ms/step - loss: 3.5574 - accuracy: 0.1268 - val_loss: 64.5107 - val_accuracy: 0.0493\n",
      "Epoch 16/25\n",
      "1145/1145 [==============================] - 73s 64ms/step - loss: 3.5996 - accuracy: 0.1379 - val_loss: 117.0594 - val_accuracy: 0.0617\n",
      "Epoch 17/25\n",
      "1145/1145 [==============================] - 74s 64ms/step - loss: 3.5719 - accuracy: 0.1278 - val_loss: 227.8199 - val_accuracy: 0.0663\n",
      "Epoch 18/25\n",
      "1145/1145 [==============================] - 74s 64ms/step - loss: 3.5614 - accuracy: 0.1304 - val_loss: 247.5011 - val_accuracy: 0.0666\n",
      "Epoch 19/25\n",
      "1145/1145 [==============================] - 76s 66ms/step - loss: 3.6342 - accuracy: 0.1294 - val_loss: 249.2863 - val_accuracy: 0.0606\n",
      "Epoch 20/25\n",
      "1145/1145 [==============================] - 77s 67ms/step - loss: 3.6075 - accuracy: 0.1297 - val_loss: 114.7423 - val_accuracy: 0.0660\n",
      "Epoch 21/25\n",
      "1145/1145 [==============================] - 77s 67ms/step - loss: 3.5735 - accuracy: 0.1386 - val_loss: 152.0242 - val_accuracy: 0.0501\n",
      "Epoch 22/25\n",
      "1145/1145 [==============================] - 74s 65ms/step - loss: 3.5878 - accuracy: 0.1246 - val_loss: 148.4701 - val_accuracy: 0.0671\n",
      "Epoch 23/25\n",
      "1145/1145 [==============================] - 74s 65ms/step - loss: 3.6005 - accuracy: 0.1304 - val_loss: 142.6960 - val_accuracy: 0.0609\n",
      "Epoch 24/25\n",
      "1145/1145 [==============================] - 74s 65ms/step - loss: 3.6053 - accuracy: 0.1307 - val_loss: 237.4102 - val_accuracy: 0.0582\n",
      "Epoch 25/25\n",
      "1145/1145 [==============================] - 74s 65ms/step - loss: 3.6182 - accuracy: 0.1290 - val_loss: 146.2400 - val_accuracy: 0.0666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a9d7cc40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=25,\n",
    "          validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59edb70e",
   "metadata": {},
   "source": [
    "### replace flatten with global average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "214a629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2,\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe655f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 47, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 47, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 45, 46, 4)         388       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 45, 46, 4)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 104)               6760      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 104)               0         \n",
      "=================================================================\n",
      "Total params: 12,636\n",
      "Trainable params: 12,572\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.70, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.Conv2D(4, (3,2), activation='relu'))\n",
    "model.add(layers.Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "208ec3bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2fe7868b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2fe7868b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - ETA: 0s - loss: 4.1093 - accuracy: 0.0680WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2fe782550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2fe782550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - 76s 66ms/step - loss: 4.1090 - accuracy: 0.0680 - val_loss: 261.3221 - val_accuracy: 0.1038\n",
      "Epoch 2/25\n",
      "1145/1145 [==============================] - 76s 66ms/step - loss: 3.4589 - accuracy: 0.1469 - val_loss: 263.5721 - val_accuracy: 0.1067\n",
      "Epoch 3/25\n",
      "1145/1145 [==============================] - 77s 67ms/step - loss: 3.3183 - accuracy: 0.1624 - val_loss: 279.4435 - val_accuracy: 0.1065\n",
      "Epoch 4/25\n",
      "1145/1145 [==============================] - 75s 65ms/step - loss: 3.2902 - accuracy: 0.1646 - val_loss: 272.5081 - val_accuracy: 0.1186\n",
      "Epoch 5/25\n",
      "1145/1145 [==============================] - 75s 65ms/step - loss: 3.2164 - accuracy: 0.1731 - val_loss: 403.3244 - val_accuracy: 0.0984\n",
      "Epoch 6/25\n",
      "1145/1145 [==============================] - 76s 66ms/step - loss: 3.2312 - accuracy: 0.1750 - val_loss: 275.3152 - val_accuracy: 0.1439\n",
      "Epoch 7/25\n",
      "1145/1145 [==============================] - 76s 66ms/step - loss: 3.1929 - accuracy: 0.1782 - val_loss: 435.3306 - val_accuracy: 0.0927\n",
      "Epoch 8/25\n",
      "1145/1145 [==============================] - 75s 66ms/step - loss: 3.1381 - accuracy: 0.1935 - val_loss: 370.6629 - val_accuracy: 0.1108\n",
      "Epoch 9/25\n",
      "1145/1145 [==============================] - 75s 66ms/step - loss: 3.1340 - accuracy: 0.1878 - val_loss: 319.3524 - val_accuracy: 0.1135\n",
      "Epoch 10/25\n",
      "1145/1145 [==============================] - 76s 66ms/step - loss: 3.1186 - accuracy: 0.1982 - val_loss: 494.7249 - val_accuracy: 0.1011\n",
      "Epoch 11/25\n",
      "1145/1145 [==============================] - 76s 66ms/step - loss: 3.1027 - accuracy: 0.1958 - val_loss: 435.5752 - val_accuracy: 0.0598\n",
      "Epoch 12/25\n",
      "1145/1145 [==============================] - 75s 65ms/step - loss: 3.0741 - accuracy: 0.2038 - val_loss: 397.6430 - val_accuracy: 0.0962\n",
      "Epoch 13/25\n",
      "1145/1145 [==============================] - 75s 66ms/step - loss: 3.0756 - accuracy: 0.2022 - val_loss: 263.3274 - val_accuracy: 0.1191\n",
      "Epoch 14/25\n",
      "1145/1145 [==============================] - 77s 68ms/step - loss: 3.0528 - accuracy: 0.2057 - val_loss: 374.7185 - val_accuracy: 0.1024\n",
      "Epoch 15/25\n",
      "1145/1145 [==============================] - 75s 65ms/step - loss: 3.0240 - accuracy: 0.2075 - val_loss: 371.3446 - val_accuracy: 0.1000\n",
      "Epoch 16/25\n",
      "1145/1145 [==============================] - 76s 66ms/step - loss: 3.0363 - accuracy: 0.2122 - val_loss: 473.1928 - val_accuracy: 0.0760\n",
      "Epoch 17/25\n",
      "1145/1145 [==============================] - 73s 64ms/step - loss: 3.0359 - accuracy: 0.2076 - val_loss: 357.3557 - val_accuracy: 0.0887\n",
      "Epoch 18/25\n",
      "1145/1145 [==============================] - 75s 66ms/step - loss: 2.9961 - accuracy: 0.2231 - val_loss: 523.6544 - val_accuracy: 0.0712\n",
      "Epoch 19/25\n",
      "1145/1145 [==============================] - 78s 68ms/step - loss: 2.9751 - accuracy: 0.2225 - val_loss: 525.1722 - val_accuracy: 0.0644\n",
      "Epoch 20/25\n",
      "1145/1145 [==============================] - 79s 69ms/step - loss: 2.9864 - accuracy: 0.2142 - val_loss: 547.3213 - val_accuracy: 0.0609\n",
      "Epoch 21/25\n",
      "1145/1145 [==============================] - 77s 67ms/step - loss: 2.9781 - accuracy: 0.2133 - val_loss: 431.2943 - val_accuracy: 0.1035\n",
      "Epoch 22/25\n",
      "1145/1145 [==============================] - 78s 68ms/step - loss: 2.9444 - accuracy: 0.2203 - val_loss: 472.2821 - val_accuracy: 0.0846\n",
      "Epoch 23/25\n",
      "1145/1145 [==============================] - 79s 69ms/step - loss: 2.9242 - accuracy: 0.2178 - val_loss: 588.1907 - val_accuracy: 0.0811\n",
      "Epoch 24/25\n",
      "1145/1145 [==============================] - 80s 70ms/step - loss: 2.9531 - accuracy: 0.2202 - val_loss: 647.2532 - val_accuracy: 0.0625\n",
      "Epoch 25/25\n",
      "1145/1145 [==============================] - 81s 71ms/step - loss: 2.9618 - accuracy: 0.2223 - val_loss: 432.9597 - val_accuracy: 0.0666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2fe2a70a0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=25,\n",
    "          validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948813b6",
   "metadata": {},
   "source": [
    "### rerunning this in hopes to get best performing model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7456c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2,\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00adbbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 95, 95, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 47, 47, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 46, 46, 64)        16448     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 135424)            0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 135424)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8667200   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 104)               6760      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 104)               0         \n",
      "=================================================================\n",
      "Total params: 8,699,080\n",
      "Trainable params: 8,699,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (2,2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (2,2), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be427fc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2fe870a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2fe870a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - ETA: 0s - loss: 4.1307 - accuracy: 0.0758WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2cf9f4d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2cf9f4d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - 109s 95ms/step - loss: 4.1304 - accuracy: 0.0758 - val_loss: 203.2040 - val_accuracy: 0.1186\n",
      "Epoch 2/25\n",
      "1145/1145 [==============================] - 112s 97ms/step - loss: 3.4339 - accuracy: 0.1609 - val_loss: 235.8162 - val_accuracy: 0.1186\n",
      "Epoch 3/25\n",
      "1145/1145 [==============================] - 119s 104ms/step - loss: 3.1897 - accuracy: 0.1982 - val_loss: 284.8329 - val_accuracy: 0.1569\n",
      "Epoch 4/25\n",
      "1145/1145 [==============================] - 114s 100ms/step - loss: 3.0643 - accuracy: 0.2256 - val_loss: 225.3302 - val_accuracy: 0.1811\n",
      "Epoch 5/25\n",
      "1145/1145 [==============================] - 112s 98ms/step - loss: 2.9476 - accuracy: 0.2365 - val_loss: 172.3603 - val_accuracy: 0.1903\n",
      "Epoch 6/25\n",
      "1145/1145 [==============================] - 107s 93ms/step - loss: 2.8611 - accuracy: 0.2552 - val_loss: 228.6638 - val_accuracy: 0.1941\n",
      "Epoch 7/25\n",
      "1145/1145 [==============================] - 101s 88ms/step - loss: 2.8126 - accuracy: 0.2728 - val_loss: 201.8111 - val_accuracy: 0.1809\n",
      "Epoch 8/25\n",
      "1145/1145 [==============================] - 100s 88ms/step - loss: 2.7538 - accuracy: 0.2784 - val_loss: 205.6072 - val_accuracy: 0.1647\n",
      "Epoch 9/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.7101 - accuracy: 0.2893 - val_loss: 191.5348 - val_accuracy: 0.2105\n",
      "Epoch 10/25\n",
      "1145/1145 [==============================] - 98s 86ms/step - loss: 2.6722 - accuracy: 0.2975 - val_loss: 170.5408 - val_accuracy: 0.2062\n",
      "Epoch 11/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.6338 - accuracy: 0.3017 - val_loss: 154.8050 - val_accuracy: 0.1992\n",
      "Epoch 12/25\n",
      "1145/1145 [==============================] - 101s 88ms/step - loss: 2.6161 - accuracy: 0.3171 - val_loss: 187.8258 - val_accuracy: 0.1903\n",
      "Epoch 13/25\n",
      "1145/1145 [==============================] - 98s 86ms/step - loss: 2.6005 - accuracy: 0.3181 - val_loss: 158.4048 - val_accuracy: 0.1903\n",
      "Epoch 14/25\n",
      "1145/1145 [==============================] - 98s 86ms/step - loss: 2.5421 - accuracy: 0.3314 - val_loss: 184.7882 - val_accuracy: 0.2051\n",
      "Epoch 15/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.5334 - accuracy: 0.3292 - val_loss: 187.4083 - val_accuracy: 0.2318\n",
      "Epoch 16/25\n",
      "1145/1145 [==============================] - 100s 88ms/step - loss: 2.4901 - accuracy: 0.3429 - val_loss: 232.2639 - val_accuracy: 0.1814\n",
      "Epoch 17/25\n",
      "1145/1145 [==============================] - 100s 88ms/step - loss: 2.4738 - accuracy: 0.3414 - val_loss: 224.5862 - val_accuracy: 0.2057\n",
      "Epoch 18/25\n",
      "1145/1145 [==============================] - 100s 87ms/step - loss: 2.5183 - accuracy: 0.3439 - val_loss: 204.8527 - val_accuracy: 0.2256\n",
      "Epoch 19/25\n",
      "1145/1145 [==============================] - 100s 87ms/step - loss: 2.4921 - accuracy: 0.3351 - val_loss: 155.2117 - val_accuracy: 0.2178\n",
      "Epoch 20/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.4691 - accuracy: 0.3417 - val_loss: 169.4785 - val_accuracy: 0.2307\n",
      "Epoch 21/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.4268 - accuracy: 0.3516 - val_loss: 218.8454 - val_accuracy: 0.2391\n",
      "Epoch 22/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.4276 - accuracy: 0.3546 - val_loss: 237.4671 - val_accuracy: 0.2283\n",
      "Epoch 23/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.4352 - accuracy: 0.3478 - val_loss: 202.7003 - val_accuracy: 0.2520\n",
      "Epoch 24/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.4337 - accuracy: 0.3526 - val_loss: 241.5366 - val_accuracy: 0.2024\n",
      "Epoch 25/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.3984 - accuracy: 0.3635 - val_loss: 236.1217 - val_accuracy: 0.2008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2aab33790>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=25,\n",
    "          validation_data=val_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608414bb",
   "metadata": {},
   "source": [
    "### running above with learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bae8d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5da25dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1b190a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 95, 95, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 47, 47, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 46, 46, 64)        16448     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 135424)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 135424)            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                8667200   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 104)               6760      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 104)               0         \n",
      "=================================================================\n",
      "Total params: 8,699,080\n",
      "Trainable params: 8,699,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (2,2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (2,2), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0aea0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d00779d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2d00779d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - ETA: 0s - loss: 4.0301 - accuracy: 0.0905WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2fefb1ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2fefb1ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 4.0299 - accuracy: 0.0905 - val_loss: 151.7955 - val_accuracy: 0.1353\n",
      "Epoch 2/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 3.3704 - accuracy: 0.1702 - val_loss: 176.2399 - val_accuracy: 0.1706\n",
      "Epoch 3/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 3.1685 - accuracy: 0.1914 - val_loss: 215.1403 - val_accuracy: 0.1873\n",
      "Epoch 4/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 3.0643 - accuracy: 0.2102 - val_loss: 185.4122 - val_accuracy: 0.1916\n",
      "Epoch 5/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.9532 - accuracy: 0.2399 - val_loss: 273.9863 - val_accuracy: 0.1523\n",
      "Epoch 6/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.8342 - accuracy: 0.2569 - val_loss: 264.5327 - val_accuracy: 0.1620\n",
      "Epoch 7/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.7401 - accuracy: 0.2720 - val_loss: 223.7726 - val_accuracy: 0.1857\n",
      "Epoch 8/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.6996 - accuracy: 0.2874 - val_loss: 202.9182 - val_accuracy: 0.1951\n",
      "Epoch 9/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.6630 - accuracy: 0.2934 - val_loss: 221.0086 - val_accuracy: 0.1946\n",
      "Epoch 10/25\n",
      "1145/1145 [==============================] - 98s 86ms/step - loss: 2.6293 - accuracy: 0.3007 - val_loss: 225.0456 - val_accuracy: 0.2005\n",
      "Epoch 11/25\n",
      "1145/1145 [==============================] - 98s 86ms/step - loss: 2.5571 - accuracy: 0.3173 - val_loss: 196.7980 - val_accuracy: 0.2016\n",
      "Epoch 12/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.5245 - accuracy: 0.3340 - val_loss: 218.1291 - val_accuracy: 0.1811\n",
      "Epoch 13/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.4802 - accuracy: 0.3416 - val_loss: 287.6992 - val_accuracy: 0.1615\n",
      "Epoch 14/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.4776 - accuracy: 0.3432 - val_loss: 286.5100 - val_accuracy: 0.1744\n",
      "Epoch 15/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.4166 - accuracy: 0.3530 - val_loss: 263.8869 - val_accuracy: 0.1698\n",
      "Epoch 16/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.3782 - accuracy: 0.3625 - val_loss: 266.7283 - val_accuracy: 0.1747\n",
      "Epoch 17/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.3679 - accuracy: 0.3658 - val_loss: 279.1163 - val_accuracy: 0.1682\n",
      "Epoch 18/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.3002 - accuracy: 0.3831 - val_loss: 265.1566 - val_accuracy: 0.1811\n",
      "Epoch 19/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.2824 - accuracy: 0.3877 - val_loss: 272.1034 - val_accuracy: 0.2038\n",
      "Epoch 20/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.2793 - accuracy: 0.3820 - val_loss: 315.4775 - val_accuracy: 0.1806\n",
      "Epoch 21/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.2412 - accuracy: 0.3854 - val_loss: 293.5861 - val_accuracy: 0.1927\n",
      "Epoch 22/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.2466 - accuracy: 0.3992 - val_loss: 280.2253 - val_accuracy: 0.2057\n",
      "Epoch 23/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.1956 - accuracy: 0.4045 - val_loss: 238.6762 - val_accuracy: 0.2065\n",
      "Epoch 24/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.1743 - accuracy: 0.4053 - val_loss: 260.5864 - val_accuracy: 0.1973\n",
      "Epoch 25/25\n",
      "1145/1145 [==============================] - 99s 86ms/step - loss: 2.2001 - accuracy: 0.4092 - val_loss: 271.5286 - val_accuracy: 0.2202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a73c7d30>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=25,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7b41f",
   "metadata": {},
   "source": [
    "### increasing number of epochs for the sake of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d08fcd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a942163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1263e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 95, 95, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 47, 47, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 46, 46, 64)        16448     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 135424)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 135424)            0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                8667200   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 104)               3432      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 104)               0         \n",
      "=================================================================\n",
      "Total params: 8,697,960\n",
      "Trainable params: 8,697,896\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (2,2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (2,2), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "374f40d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x306086310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x306086310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - ETA: 0s - loss: 4.2746 - accuracy: 0.0681WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2cf9f4040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2cf9f4040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 4.2744 - accuracy: 0.0681 - val_loss: 195.6339 - val_accuracy: 0.0774\n",
      "Epoch 2/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 3.5497 - accuracy: 0.1526 - val_loss: 228.1601 - val_accuracy: 0.1005\n",
      "Epoch 3/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 3.3376 - accuracy: 0.1784 - val_loss: 127.5001 - val_accuracy: 0.1051\n",
      "Epoch 4/50\n",
      "1145/1145 [==============================] - 101s 88ms/step - loss: 3.1966 - accuracy: 0.1914 - val_loss: 222.9627 - val_accuracy: 0.1496\n",
      "Epoch 5/50\n",
      "1145/1145 [==============================] - 101s 89ms/step - loss: 3.0969 - accuracy: 0.1986 - val_loss: 256.0226 - val_accuracy: 0.1108\n",
      "Epoch 6/50\n",
      "1145/1145 [==============================] - 101s 89ms/step - loss: 3.0453 - accuracy: 0.2089 - val_loss: 192.3036 - val_accuracy: 0.1580\n",
      "Epoch 7/50\n",
      "1145/1145 [==============================] - 101s 88ms/step - loss: 2.9936 - accuracy: 0.2268 - val_loss: 180.7772 - val_accuracy: 0.1332\n",
      "Epoch 8/50\n",
      "1145/1145 [==============================] - 101s 89ms/step - loss: 2.9609 - accuracy: 0.2256 - val_loss: 257.1698 - val_accuracy: 0.1243\n",
      "Epoch 9/50\n",
      "1145/1145 [==============================] - 101s 89ms/step - loss: 2.9320 - accuracy: 0.2385 - val_loss: 194.5499 - val_accuracy: 0.1275\n",
      "Epoch 10/50\n",
      "1145/1145 [==============================] - 101s 88ms/step - loss: 2.9037 - accuracy: 0.2386 - val_loss: 128.2830 - val_accuracy: 0.1650\n",
      "Epoch 11/50\n",
      "1145/1145 [==============================] - 101s 89ms/step - loss: 2.8767 - accuracy: 0.2451 - val_loss: 161.2448 - val_accuracy: 0.1038\n",
      "Epoch 12/50\n",
      "1145/1145 [==============================] - 101s 89ms/step - loss: 2.8188 - accuracy: 0.2519 - val_loss: 159.3591 - val_accuracy: 0.1156\n",
      "Epoch 13/50\n",
      "1145/1145 [==============================] - 101s 89ms/step - loss: 2.7558 - accuracy: 0.2737 - val_loss: 138.0059 - val_accuracy: 0.1259\n",
      "Epoch 14/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.7429 - accuracy: 0.2687 - val_loss: 91.5898 - val_accuracy: 0.1108\n",
      "Epoch 15/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.6974 - accuracy: 0.2818 - val_loss: 124.5240 - val_accuracy: 0.1569\n",
      "Epoch 16/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.6544 - accuracy: 0.2906 - val_loss: 182.3376 - val_accuracy: 0.1464\n",
      "Epoch 17/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.6418 - accuracy: 0.2953 - val_loss: 147.9471 - val_accuracy: 0.1881\n",
      "Epoch 18/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.6029 - accuracy: 0.3072 - val_loss: 153.8720 - val_accuracy: 0.1795\n",
      "Epoch 19/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.5501 - accuracy: 0.3136 - val_loss: 167.4666 - val_accuracy: 0.1997\n",
      "Epoch 20/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.5423 - accuracy: 0.3216 - val_loss: 170.0822 - val_accuracy: 0.1558\n",
      "Epoch 21/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.5299 - accuracy: 0.3192 - val_loss: 152.1707 - val_accuracy: 0.2003\n",
      "Epoch 22/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.4961 - accuracy: 0.3326 - val_loss: 139.9490 - val_accuracy: 0.2005\n",
      "Epoch 23/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.4627 - accuracy: 0.3255 - val_loss: 162.7338 - val_accuracy: 0.2245\n",
      "Epoch 24/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.4612 - accuracy: 0.3303 - val_loss: 159.4128 - val_accuracy: 0.2588\n",
      "Epoch 25/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.4292 - accuracy: 0.3396 - val_loss: 179.2935 - val_accuracy: 0.2229\n",
      "Epoch 26/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.4304 - accuracy: 0.3397 - val_loss: 162.2899 - val_accuracy: 0.2278\n",
      "Epoch 27/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.4265 - accuracy: 0.3392 - val_loss: 196.2229 - val_accuracy: 0.1887\n",
      "Epoch 28/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3993 - accuracy: 0.3537 - val_loss: 176.5646 - val_accuracy: 0.2119\n",
      "Epoch 29/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.4282 - accuracy: 0.3392 - val_loss: 160.1967 - val_accuracy: 0.2350\n",
      "Epoch 30/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3907 - accuracy: 0.3495 - val_loss: 182.4824 - val_accuracy: 0.2127\n",
      "Epoch 31/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3784 - accuracy: 0.3528 - val_loss: 157.8200 - val_accuracy: 0.2084\n",
      "Epoch 32/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3831 - accuracy: 0.3534 - val_loss: 183.5473 - val_accuracy: 0.2264\n",
      "Epoch 33/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3324 - accuracy: 0.3625 - val_loss: 170.8540 - val_accuracy: 0.2243\n",
      "Epoch 34/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3448 - accuracy: 0.3589 - val_loss: 184.8492 - val_accuracy: 0.2084\n",
      "Epoch 35/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3496 - accuracy: 0.3609 - val_loss: 169.2857 - val_accuracy: 0.2248\n",
      "Epoch 36/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3200 - accuracy: 0.3612 - val_loss: 194.4808 - val_accuracy: 0.2062\n",
      "Epoch 37/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3314 - accuracy: 0.3600 - val_loss: 188.5077 - val_accuracy: 0.2181\n",
      "Epoch 38/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3142 - accuracy: 0.3718 - val_loss: 175.9889 - val_accuracy: 0.2151\n",
      "Epoch 39/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.2816 - accuracy: 0.3772 - val_loss: 185.2301 - val_accuracy: 0.2151\n",
      "Epoch 40/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.2968 - accuracy: 0.3631 - val_loss: 178.8918 - val_accuracy: 0.2194\n",
      "Epoch 41/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3335 - accuracy: 0.3635 - val_loss: 191.2257 - val_accuracy: 0.2108\n",
      "Epoch 42/50\n",
      "1145/1145 [==============================] - 103s 90ms/step - loss: 2.3010 - accuracy: 0.3725 - val_loss: 193.1470 - val_accuracy: 0.2186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3088 - accuracy: 0.3728 - val_loss: 183.7158 - val_accuracy: 0.2170\n",
      "Epoch 44/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.2849 - accuracy: 0.3732 - val_loss: 174.5177 - val_accuracy: 0.2323\n",
      "Epoch 45/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3005 - accuracy: 0.3690 - val_loss: 178.0510 - val_accuracy: 0.2181\n",
      "Epoch 46/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.2708 - accuracy: 0.3800 - val_loss: 182.0894 - val_accuracy: 0.2159\n",
      "Epoch 47/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3004 - accuracy: 0.3659 - val_loss: 183.3173 - val_accuracy: 0.2164\n",
      "Epoch 48/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.3126 - accuracy: 0.3767 - val_loss: 186.6792 - val_accuracy: 0.2148\n",
      "Epoch 49/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.2834 - accuracy: 0.3766 - val_loss: 181.0298 - val_accuracy: 0.2240\n",
      "Epoch 50/50\n",
      "1145/1145 [==============================] - 102s 89ms/step - loss: 2.2942 - accuracy: 0.3815 - val_loss: 178.5520 - val_accuracy: 0.2208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3060b5a30>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=50,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b8f77",
   "metadata": {},
   "source": [
    "### trying to recreate the best model. reducing number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5180c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e586b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=90,\n",
    "                                     vertical_flip=True,\n",
    "                                     brightness_range=[0.5,1.5],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37d1a299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 94, 94, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 31, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 45632)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 45632)             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                1460256   \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 104)               3432      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 104)               0         \n",
      "=================================================================\n",
      "Total params: 1,482,600\n",
      "Trainable params: 1,482,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,4)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3806b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x30581a8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x30581a8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - ETA: 0s - loss: 4.0908 - accuracy: 0.0856WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x3962c1c10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x3962c1c10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1145/1145 [==============================] - 94s 82ms/step - loss: 4.0906 - accuracy: 0.0856 - val_loss: 208.0112 - val_accuracy: 0.1358\n",
      "Epoch 2/50\n",
      "1145/1145 [==============================] - 94s 82ms/step - loss: 3.3691 - accuracy: 0.1724 - val_loss: 243.4412 - val_accuracy: 0.1884\n",
      "Epoch 3/50\n",
      "1145/1145 [==============================] - 91s 79ms/step - loss: 3.1471 - accuracy: 0.2075 - val_loss: 238.0947 - val_accuracy: 0.1962\n",
      "Epoch 4/50\n",
      "1145/1145 [==============================] - 91s 79ms/step - loss: 3.0231 - accuracy: 0.2283 - val_loss: 232.7818 - val_accuracy: 0.1811\n",
      "Epoch 5/50\n",
      "1145/1145 [==============================] - 91s 80ms/step - loss: 2.9422 - accuracy: 0.2388 - val_loss: 235.1550 - val_accuracy: 0.1997\n",
      "Epoch 6/50\n",
      "1145/1145 [==============================] - 92s 80ms/step - loss: 2.8512 - accuracy: 0.2590 - val_loss: 231.3258 - val_accuracy: 0.2167\n",
      "Epoch 7/50\n",
      "1145/1145 [==============================] - 92s 80ms/step - loss: 2.8101 - accuracy: 0.2604 - val_loss: 217.6885 - val_accuracy: 0.2305\n",
      "Epoch 8/50\n",
      "1145/1145 [==============================] - 93s 82ms/step - loss: 2.7688 - accuracy: 0.2741 - val_loss: 228.9598 - val_accuracy: 0.2299\n",
      "Epoch 9/50\n",
      "1145/1145 [==============================] - 93s 81ms/step - loss: 2.7317 - accuracy: 0.2828 - val_loss: 223.1283 - val_accuracy: 0.2439\n",
      "Epoch 10/50\n",
      "1145/1145 [==============================] - 93s 81ms/step - loss: 2.6705 - accuracy: 0.2983 - val_loss: 218.6030 - val_accuracy: 0.2129\n",
      "Epoch 11/50\n",
      "1145/1145 [==============================] - 91s 79ms/step - loss: 2.6472 - accuracy: 0.3041 - val_loss: 234.9631 - val_accuracy: 0.2170\n",
      "Epoch 12/50\n",
      "1145/1145 [==============================] - 92s 80ms/step - loss: 2.6117 - accuracy: 0.3135 - val_loss: 210.5100 - val_accuracy: 0.2159\n",
      "Epoch 13/50\n",
      "1145/1145 [==============================] - 91s 79ms/step - loss: 2.5480 - accuracy: 0.3185 - val_loss: 223.8887 - val_accuracy: 0.2480\n",
      "Epoch 14/50\n",
      "1145/1145 [==============================] - 91s 79ms/step - loss: 2.5593 - accuracy: 0.3233 - val_loss: 237.3100 - val_accuracy: 0.1784\n",
      "Epoch 15/50\n",
      "1145/1145 [==============================] - 91s 80ms/step - loss: 2.5372 - accuracy: 0.3333 - val_loss: 224.8083 - val_accuracy: 0.2075\n",
      "Epoch 16/50\n",
      "1145/1145 [==============================] - 91s 79ms/step - loss: 2.5093 - accuracy: 0.3363 - val_loss: 228.0128 - val_accuracy: 0.2197\n",
      "Epoch 17/50\n",
      "1145/1145 [==============================] - 91s 79ms/step - loss: 2.4735 - accuracy: 0.3376 - val_loss: 244.7263 - val_accuracy: 0.1884\n",
      "Epoch 18/50\n",
      "1145/1145 [==============================] - 90s 79ms/step - loss: 2.4623 - accuracy: 0.3437 - val_loss: 228.8602 - val_accuracy: 0.2216\n",
      "Epoch 19/50\n",
      "1145/1145 [==============================] - 92s 80ms/step - loss: 2.4218 - accuracy: 0.3470 - val_loss: 247.2881 - val_accuracy: 0.2167\n",
      "Epoch 20/50\n",
      "1145/1145 [==============================] - 92s 81ms/step - loss: 2.4339 - accuracy: 0.3506 - val_loss: 229.4265 - val_accuracy: 0.2323\n",
      "Epoch 21/50\n",
      "1145/1145 [==============================] - 93s 81ms/step - loss: 2.4412 - accuracy: 0.3488 - val_loss: 248.8943 - val_accuracy: 0.2305\n",
      "Epoch 22/50\n",
      "1145/1145 [==============================] - 92s 80ms/step - loss: 2.4188 - accuracy: 0.3578 - val_loss: 242.5019 - val_accuracy: 0.2229\n",
      "Epoch 23/50\n",
      "1145/1145 [==============================] - 96s 84ms/step - loss: 2.3900 - accuracy: 0.3651 - val_loss: 258.2390 - val_accuracy: 0.2159\n",
      "Epoch 24/50\n",
      "1145/1145 [==============================] - 93s 82ms/step - loss: 2.3962 - accuracy: 0.3612 - val_loss: 252.5932 - val_accuracy: 0.2075\n",
      "Epoch 25/50\n",
      " 198/1145 [====>.........................] - ETA: 1:12 - loss: 2.3251 - accuracy: 0.3755"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_13678/302656601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=50,\n\u001b[0m\u001b[1;32m      2\u001b[0m           validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11459 // BATCH_SIZE, epochs=50,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1293b",
   "metadata": {},
   "source": [
    "### attempting to reduce the number of classes. removing the classes beyond three standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d98ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d0b6173b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/jpeg-192x192/train/toad lily',\n",
       " 'data/jpeg-192x192/train/love in the mist',\n",
       " 'data/jpeg-192x192/train/monkshood',\n",
       " 'data/jpeg-192x192/train/azalea',\n",
       " 'data/jpeg-192x192/train/fritillary',\n",
       " 'data/jpeg-192x192/train/silverbush',\n",
       " 'data/jpeg-192x192/train/canterbury bells',\n",
       " 'data/jpeg-192x192/train/stemless gentian',\n",
       " 'data/jpeg-192x192/train/pink primrose',\n",
       " 'data/jpeg-192x192/train/buttercup',\n",
       " 'data/jpeg-192x192/train/poinsettia',\n",
       " 'data/jpeg-192x192/train/desert-rose',\n",
       " 'data/jpeg-192x192/train/bird of paradise',\n",
       " 'data/jpeg-192x192/train/columbine',\n",
       " 'data/jpeg-192x192/train/frangipani',\n",
       " 'data/jpeg-192x192/train/sweet pea',\n",
       " 'data/jpeg-192x192/train/siam tulip',\n",
       " 'data/jpeg-192x192/train/great masterwort',\n",
       " 'data/jpeg-192x192/train/hard-leaved pocket orchid',\n",
       " 'data/jpeg-192x192/train/marigold',\n",
       " 'data/jpeg-192x192/train/foxglove',\n",
       " 'data/jpeg-192x192/train/wild pansy',\n",
       " 'data/jpeg-192x192/train/windflower',\n",
       " 'data/jpeg-192x192/train/daisy',\n",
       " 'data/jpeg-192x192/train/tiger lily',\n",
       " 'data/jpeg-192x192/train/purple coneflower',\n",
       " 'data/jpeg-192x192/train/orange dahlia',\n",
       " 'data/jpeg-192x192/train/globe-flower',\n",
       " 'data/jpeg-192x192/train/lilac hibiscus',\n",
       " 'data/jpeg-192x192/train/fire lily',\n",
       " 'data/jpeg-192x192/train/balloon flower',\n",
       " 'data/jpeg-192x192/train/iris',\n",
       " 'data/jpeg-192x192/train/bishop of llandaff',\n",
       " 'data/jpeg-192x192/train/yellow iris',\n",
       " 'data/jpeg-192x192/train/garden phlox',\n",
       " 'data/jpeg-192x192/train/alpine sea holly',\n",
       " 'data/jpeg-192x192/train/geranium',\n",
       " 'data/jpeg-192x192/train/pink quill',\n",
       " 'data/jpeg-192x192/train/tree poppy',\n",
       " 'data/jpeg-192x192/train/spear thistle',\n",
       " 'data/jpeg-192x192/train/bromelia',\n",
       " 'data/jpeg-192x192/train/common dandelion',\n",
       " 'data/jpeg-192x192/train/sword lily',\n",
       " 'data/jpeg-192x192/train/peruvian lily',\n",
       " 'data/jpeg-192x192/train/carnation',\n",
       " 'data/jpeg-192x192/train/cosmos',\n",
       " 'data/jpeg-192x192/train/spring crocus',\n",
       " 'data/jpeg-192x192/train/lotus',\n",
       " 'data/jpeg-192x192/train/bolero deep blue',\n",
       " 'data/jpeg-192x192/train/anthurium',\n",
       " 'data/jpeg-192x192/train/rose',\n",
       " 'data/jpeg-192x192/train/water lily',\n",
       " 'data/jpeg-192x192/train/primula',\n",
       " 'data/jpeg-192x192/train/blackberry lily',\n",
       " 'data/jpeg-192x192/train/gaura',\n",
       " 'data/jpeg-192x192/train/trumpet creeper',\n",
       " 'data/jpeg-192x192/train/globe thistle',\n",
       " 'data/jpeg-192x192/train/sweet william',\n",
       " 'data/jpeg-192x192/train/snapdragon',\n",
       " 'data/jpeg-192x192/train/mexican petunia',\n",
       " 'data/jpeg-192x192/train/cyclamen ',\n",
       " 'data/jpeg-192x192/train/petunia',\n",
       " 'data/jpeg-192x192/train/gazania',\n",
       " 'data/jpeg-192x192/train/king protea',\n",
       " 'data/jpeg-192x192/train/blanket flower',\n",
       " 'data/jpeg-192x192/train/common tulip',\n",
       " 'data/jpeg-192x192/train/giant white arum lily',\n",
       " 'data/jpeg-192x192/train/wild rose',\n",
       " 'data/jpeg-192x192/train/morning glory',\n",
       " 'data/jpeg-192x192/train/thorn apple',\n",
       " 'data/jpeg-192x192/train/pincushion flower',\n",
       " 'data/jpeg-192x192/train/tree mallow',\n",
       " 'data/jpeg-192x192/train/canna lily',\n",
       " 'data/jpeg-192x192/train/camellia',\n",
       " 'data/jpeg-192x192/train/pink-yellow dahlia',\n",
       " 'data/jpeg-192x192/train/bee balm',\n",
       " 'data/jpeg-192x192/train/wild geranium',\n",
       " 'data/jpeg-192x192/train/artichoke',\n",
       " 'data/jpeg-192x192/train/black-eyed susan',\n",
       " 'data/jpeg-192x192/train/ruby-lipped cattleya',\n",
       " 'data/jpeg-192x192/train/clematis',\n",
       " 'data/jpeg-192x192/train/prince of wales feathers',\n",
       " 'data/jpeg-192x192/train/hibiscus',\n",
       " 'data/jpeg-192x192/train/cautleya spicata',\n",
       " 'data/jpeg-192x192/train/lenten rose',\n",
       " 'data/jpeg-192x192/train/red ginger',\n",
       " \"data/jpeg-192x192/train/colt's foot\",\n",
       " 'data/jpeg-192x192/train/hippeastrum ',\n",
       " 'data/jpeg-192x192/train/mallow',\n",
       " 'data/jpeg-192x192/train/californian poppy',\n",
       " 'data/jpeg-192x192/train/corn poppy',\n",
       " 'data/jpeg-192x192/train/moon orchid',\n",
       " 'data/jpeg-192x192/train/passion flower',\n",
       " 'data/jpeg-192x192/train/grape hyacinth',\n",
       " 'data/jpeg-192x192/train/japanese anemone',\n",
       " 'data/jpeg-192x192/train/watercress',\n",
       " 'data/jpeg-192x192/train/cape flower',\n",
       " 'data/jpeg-192x192/train/osteospermum',\n",
       " 'data/jpeg-192x192/train/barberton daisy',\n",
       " 'data/jpeg-192x192/train/bougainvillea',\n",
       " 'data/jpeg-192x192/train/magnolia',\n",
       " 'data/jpeg-192x192/train/sunflower',\n",
       " 'data/jpeg-192x192/train/daffodil',\n",
       " 'data/jpeg-192x192/train/wallflower']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can delete\n",
    "class_list_dir = []\n",
    "\n",
    "for file in os.listdir(TRAIN_DIR):\n",
    "    d = os.path.join(TRAIN_DIR, file)\n",
    "    if os.path.isdir(d):\n",
    "        class_list_dir.append(d)\n",
    "        \n",
    "class_list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dfa02b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11564\n"
     ]
    }
   ],
   "source": [
    "file_count = sum(len(files) for _, _, files in os.walk(TRAIN_DIR))\n",
    "print(file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "071273a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can delete\n",
    "class_count = {}\n",
    "\n",
    "for class_folder in class_list_dir:\n",
    "    file_count = sum(len(files) for _, _, files in os.walk(class_folder))\n",
    "    class_count[class_folder[24:]] = file_count, class_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9889d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78, 21, 79, 124, 44, 26, 18, 32, 246, 236, 100, 29, 95, 102, 139, 19, 17, 24, 24, 113, 126, 95, 235, 381, 79, 50, 33, 19, 34, 17, 82, 707, 53, 205, 22, 17, 58, 22, 86, 238, 32, 508, 58, 46, 95, 18, 19, 126, 16, 107, 415, 151, 104, 23, 81, 29, 77, 99, 123, 38, 132, 182, 95, 84, 22, 352, 24, 670, 276, 113, 87, 27, 42, 115, 53, 101, 634, 108, 85, 33, 92, 19, 122, 25, 67, 17, 40, 96, 91, 50, 76, 16, 108, 77, 27, 87, 52, 31, 59, 119, 109, 415, 87, 156]\n"
     ]
    }
   ],
   "source": [
    "# can delete \n",
    "flower_count = []\n",
    "\n",
    "for flower in list(class_count.values()):\n",
    "    flower_count.append(flower[0])\n",
    "    \n",
    "print(flower_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0b41f2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "data/jpeg-192x192/train/toad lily\n"
     ]
    }
   ],
   "source": [
    "# shows the count\n",
    "print(list(class_count.values())[0][0])\n",
    "\n",
    "# shows the directory \n",
    "print(list(class_count.values())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b317ff59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 104 artists>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 7200x7200 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.figure(figsize=(100, 100))\n",
    "plt.bar(class_count.keys(), flower_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b6c7707e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard deviation: 132.99130714962862\n",
      "max: 707\n",
      "min: 16\n",
      "mean: 111.1826923076923\n",
      "25%: 30.5\n",
      "75%: 113.5\n",
      "10%: 19.0\n",
      "5%: 17.15\n"
     ]
    }
   ],
   "source": [
    "# finding metrics \n",
    "class_std = np.std(flower_count)\n",
    "class_max = max(flower_count)\n",
    "class_min = min(flower_count)\n",
    "class_mean = np.mean(flower_count)\n",
    "class_first_quartile = np.percentile(flower_count, 25)\n",
    "class_third_quartile = np.percentile(flower_count, 75)\n",
    "class_tenth_percentile = np.percentile(flower_count, 10)\n",
    "class_fifth_percentile = np.percentile(flower_count, 5)\n",
    "\n",
    "print(f'standard deviation: {class_std}')\n",
    "print(f'max: {class_max}')\n",
    "print(f'min: {class_min}')\n",
    "print(f'mean: {class_mean}')\n",
    "print(f'25%: {class_first_quartile}')\n",
    "print(f'75%: {class_third_quartile}')\n",
    "print(f'10%: {class_tenth_percentile}')\n",
    "print(f'5%: {class_fifth_percentile}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d998a86",
   "metadata": {},
   "source": [
    "### going to remove the folders with less than the 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4e098351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help me gottfried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d6227af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# create new directory if conditions are met\n",
    "# path of file to copy\n",
    "# path of file to create a copy\n",
    "\n",
    "src = 'data/jpeg-192x192/train/toad lily'\n",
    "dst = 'data/jpeg-192x192/train_new/toad lily'\n",
    "\n",
    "def copy_subfolder(source):\n",
    "    destination = src[:23] + '_new' + src[23:]\n",
    "    \n",
    "    shutil.copytree(source, destination, symlinks=False, ignore=None, \n",
    "                copy_function=shutil.copy2, ignore_dangling_symlinks=False, dirs_exist_ok=False)\n",
    "    \n",
    "def item_count(folder):\n",
    "    file_count = sum(len(files) for _, _, files in os.walk(folder))\n",
    "    \n",
    "    return file_count\n",
    "    \n",
    "def select_subset(subfolder_list, percentile):\n",
    "    for subfolder in subfolder_list:\n",
    "        if item_count(subfolder) < percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2723d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5c43f7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/jpeg-192x192/train/toad lily\n",
      "data/jpeg-192x192/train/love in the mist\n",
      "data/jpeg-192x192/train/monkshood\n",
      "data/jpeg-192x192/train/azalea\n",
      "data/jpeg-192x192/train/fritillary\n",
      "data/jpeg-192x192/train/silverbush\n",
      "data/jpeg-192x192/train/canterbury bells\n",
      "data/jpeg-192x192/train/stemless gentian\n",
      "data/jpeg-192x192/train/pink primrose\n",
      "data/jpeg-192x192/train/buttercup\n",
      "data/jpeg-192x192/train/poinsettia\n",
      "data/jpeg-192x192/train/desert-rose\n",
      "data/jpeg-192x192/train/bird of paradise\n",
      "data/jpeg-192x192/train/columbine\n",
      "data/jpeg-192x192/train/frangipani\n",
      "data/jpeg-192x192/train/sweet pea\n",
      "data/jpeg-192x192/train/siam tulip\n",
      "data/jpeg-192x192/train/great masterwort\n",
      "data/jpeg-192x192/train/hard-leaved pocket orchid\n",
      "data/jpeg-192x192/train/marigold\n",
      "data/jpeg-192x192/train/foxglove\n",
      "data/jpeg-192x192/train/wild pansy\n",
      "data/jpeg-192x192/train/windflower\n",
      "data/jpeg-192x192/train/daisy\n",
      "data/jpeg-192x192/train/tiger lily\n",
      "data/jpeg-192x192/train/purple coneflower\n",
      "data/jpeg-192x192/train/orange dahlia\n",
      "data/jpeg-192x192/train/globe-flower\n",
      "data/jpeg-192x192/train/lilac hibiscus\n",
      "data/jpeg-192x192/train/fire lily\n",
      "data/jpeg-192x192/train/balloon flower\n",
      "data/jpeg-192x192/train/iris\n",
      "data/jpeg-192x192/train/bishop of llandaff\n",
      "data/jpeg-192x192/train/yellow iris\n",
      "data/jpeg-192x192/train/garden phlox\n",
      "data/jpeg-192x192/train/alpine sea holly\n",
      "data/jpeg-192x192/train/geranium\n",
      "data/jpeg-192x192/train/pink quill\n",
      "data/jpeg-192x192/train/tree poppy\n",
      "data/jpeg-192x192/train/spear thistle\n",
      "data/jpeg-192x192/train/bromelia\n",
      "data/jpeg-192x192/train/common dandelion\n",
      "data/jpeg-192x192/train/sword lily\n",
      "data/jpeg-192x192/train/peruvian lily\n",
      "data/jpeg-192x192/train/carnation\n",
      "data/jpeg-192x192/train/cosmos\n",
      "data/jpeg-192x192/train/spring crocus\n",
      "data/jpeg-192x192/train/lotus\n",
      "data/jpeg-192x192/train/bolero deep blue\n",
      "data/jpeg-192x192/train/anthurium\n",
      "data/jpeg-192x192/train/rose\n",
      "data/jpeg-192x192/train/water lily\n",
      "data/jpeg-192x192/train/primula\n",
      "data/jpeg-192x192/train/blackberry lily\n",
      "data/jpeg-192x192/train/gaura\n",
      "data/jpeg-192x192/train/trumpet creeper\n",
      "data/jpeg-192x192/train/globe thistle\n",
      "data/jpeg-192x192/train/sweet william\n",
      "data/jpeg-192x192/train/snapdragon\n",
      "data/jpeg-192x192/train/mexican petunia\n",
      "data/jpeg-192x192/train/cyclamen \n",
      "data/jpeg-192x192/train/petunia\n",
      "data/jpeg-192x192/train/gazania\n",
      "data/jpeg-192x192/train/king protea\n",
      "data/jpeg-192x192/train/blanket flower\n",
      "data/jpeg-192x192/train/common tulip\n",
      "data/jpeg-192x192/train/giant white arum lily\n",
      "data/jpeg-192x192/train/wild rose\n",
      "data/jpeg-192x192/train/morning glory\n",
      "data/jpeg-192x192/train/thorn apple\n",
      "data/jpeg-192x192/train/pincushion flower\n",
      "data/jpeg-192x192/train/tree mallow\n",
      "data/jpeg-192x192/train/canna lily\n",
      "data/jpeg-192x192/train/camellia\n",
      "data/jpeg-192x192/train/pink-yellow dahlia\n",
      "data/jpeg-192x192/train/bee balm\n",
      "data/jpeg-192x192/train/wild geranium\n",
      "data/jpeg-192x192/train/artichoke\n",
      "data/jpeg-192x192/train/black-eyed susan\n",
      "data/jpeg-192x192/train/ruby-lipped cattleya\n",
      "data/jpeg-192x192/train/clematis\n",
      "data/jpeg-192x192/train/prince of wales feathers\n",
      "data/jpeg-192x192/train/hibiscus\n",
      "data/jpeg-192x192/train/cautleya spicata\n",
      "data/jpeg-192x192/train/lenten rose\n",
      "data/jpeg-192x192/train/red ginger\n",
      "data/jpeg-192x192/train/colt's foot\n",
      "data/jpeg-192x192/train/hippeastrum \n",
      "data/jpeg-192x192/train/mallow\n",
      "data/jpeg-192x192/train/californian poppy\n",
      "data/jpeg-192x192/train/corn poppy\n",
      "data/jpeg-192x192/train/moon orchid\n",
      "data/jpeg-192x192/train/passion flower\n",
      "data/jpeg-192x192/train/grape hyacinth\n",
      "data/jpeg-192x192/train/japanese anemone\n",
      "data/jpeg-192x192/train/watercress\n",
      "data/jpeg-192x192/train/cape flower\n",
      "data/jpeg-192x192/train/osteospermum\n",
      "data/jpeg-192x192/train/barberton daisy\n",
      "data/jpeg-192x192/train/bougainvillea\n",
      "data/jpeg-192x192/train/magnolia\n",
      "data/jpeg-192x192/train/sunflower\n",
      "data/jpeg-192x192/train/daffodil\n",
      "data/jpeg-192x192/train/wallflower\n"
     ]
    }
   ],
   "source": [
    "# get list of all subfolders\n",
    "def get_directory(something something something):\n",
    "    for flower in list(class_count.values()):\n",
    "        print(flower[1])\n",
    "\n",
    "# print(list(class_count.values())[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa25a5",
   "metadata": {},
   "source": [
    "## should be the start of preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ebc66",
   "metadata": {},
   "source": [
    "### adding everything into actual functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "46f6b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subfolder_list(source_folder):\n",
    "    class_list_dir = []\n",
    "\n",
    "    for file in os.listdir(source_folder):\n",
    "        d = os.path.join(source_folder, file)\n",
    "        if os.path.isdir(d):\n",
    "            class_list_dir.append(d)\n",
    "\n",
    "    return class_list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e72eb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_classes(subfolder_list):\n",
    "    class_count = {}\n",
    "\n",
    "    for class_folder in subfolder_list:\n",
    "        file_count = sum(len(files) for _, _, files in os.walk(class_folder))\n",
    "        class_count[class_folder[24:]] = file_count, class_folder\n",
    "        \n",
    "    return class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ba368653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flower_count(class_map):\n",
    "    flower_count = []\n",
    "\n",
    "    for flower in list(class_count.values()):\n",
    "        flower_count.append(flower[0])\n",
    "\n",
    "    return flower_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0c75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d5ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "82314d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new directory if conditions are met\n",
    "# path of file to copy\n",
    "# path of file to create a copy\n",
    "\n",
    "src = 'data/jpeg-192x192/train/toad lily'\n",
    "dst = 'data/jpeg-192x192/train_new/toad lily'\n",
    "\n",
    "def copy_subfolder(source):\n",
    "    destination = src[:23] + '_new' + src[23:]\n",
    "    \n",
    "    shutil.copytree(source, destination, symlinks=False, ignore=None, \n",
    "                copy_function=shutil.copy2, ignore_dangling_symlinks=False, dirs_exist_ok=False)\n",
    "    \n",
    "def item_count(folder):\n",
    "    file_count = sum(len(files) for _, _, files in os.walk(folder))\n",
    "    \n",
    "    return file_count\n",
    "    \n",
    "def select_subset(subfolder_list, percentile):\n",
    "    for subfolder in subfolder_list:\n",
    "        if item_count(subfolder) < percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "117f7166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/jpeg-192x192/train/toad lily',\n",
       " 'data/jpeg-192x192/train/love in the mist',\n",
       " 'data/jpeg-192x192/train/monkshood',\n",
       " 'data/jpeg-192x192/train/azalea',\n",
       " 'data/jpeg-192x192/train/fritillary',\n",
       " 'data/jpeg-192x192/train/silverbush',\n",
       " 'data/jpeg-192x192/train/canterbury bells',\n",
       " 'data/jpeg-192x192/train/stemless gentian',\n",
       " 'data/jpeg-192x192/train/pink primrose',\n",
       " 'data/jpeg-192x192/train/buttercup',\n",
       " 'data/jpeg-192x192/train/poinsettia',\n",
       " 'data/jpeg-192x192/train/desert-rose',\n",
       " 'data/jpeg-192x192/train/bird of paradise',\n",
       " 'data/jpeg-192x192/train/columbine',\n",
       " 'data/jpeg-192x192/train/frangipani',\n",
       " 'data/jpeg-192x192/train/sweet pea',\n",
       " 'data/jpeg-192x192/train/siam tulip',\n",
       " 'data/jpeg-192x192/train/great masterwort',\n",
       " 'data/jpeg-192x192/train/hard-leaved pocket orchid',\n",
       " 'data/jpeg-192x192/train/marigold',\n",
       " 'data/jpeg-192x192/train/foxglove',\n",
       " 'data/jpeg-192x192/train/wild pansy',\n",
       " 'data/jpeg-192x192/train/windflower',\n",
       " 'data/jpeg-192x192/train/daisy',\n",
       " 'data/jpeg-192x192/train/tiger lily',\n",
       " 'data/jpeg-192x192/train/purple coneflower',\n",
       " 'data/jpeg-192x192/train/orange dahlia',\n",
       " 'data/jpeg-192x192/train/globe-flower',\n",
       " 'data/jpeg-192x192/train/lilac hibiscus',\n",
       " 'data/jpeg-192x192/train/fire lily',\n",
       " 'data/jpeg-192x192/train/balloon flower',\n",
       " 'data/jpeg-192x192/train/iris',\n",
       " 'data/jpeg-192x192/train/bishop of llandaff',\n",
       " 'data/jpeg-192x192/train/yellow iris',\n",
       " 'data/jpeg-192x192/train/garden phlox',\n",
       " 'data/jpeg-192x192/train/alpine sea holly',\n",
       " 'data/jpeg-192x192/train/geranium',\n",
       " 'data/jpeg-192x192/train/pink quill',\n",
       " 'data/jpeg-192x192/train/tree poppy',\n",
       " 'data/jpeg-192x192/train/spear thistle',\n",
       " 'data/jpeg-192x192/train/bromelia',\n",
       " 'data/jpeg-192x192/train/common dandelion',\n",
       " 'data/jpeg-192x192/train/sword lily',\n",
       " 'data/jpeg-192x192/train/peruvian lily',\n",
       " 'data/jpeg-192x192/train/carnation',\n",
       " 'data/jpeg-192x192/train/cosmos',\n",
       " 'data/jpeg-192x192/train/spring crocus',\n",
       " 'data/jpeg-192x192/train/lotus',\n",
       " 'data/jpeg-192x192/train/bolero deep blue',\n",
       " 'data/jpeg-192x192/train/anthurium',\n",
       " 'data/jpeg-192x192/train/rose',\n",
       " 'data/jpeg-192x192/train/water lily',\n",
       " 'data/jpeg-192x192/train/primula',\n",
       " 'data/jpeg-192x192/train/blackberry lily',\n",
       " 'data/jpeg-192x192/train/gaura',\n",
       " 'data/jpeg-192x192/train/trumpet creeper',\n",
       " 'data/jpeg-192x192/train/globe thistle',\n",
       " 'data/jpeg-192x192/train/sweet william',\n",
       " 'data/jpeg-192x192/train/snapdragon',\n",
       " 'data/jpeg-192x192/train/mexican petunia',\n",
       " 'data/jpeg-192x192/train/cyclamen ',\n",
       " 'data/jpeg-192x192/train/petunia',\n",
       " 'data/jpeg-192x192/train/gazania',\n",
       " 'data/jpeg-192x192/train/king protea',\n",
       " 'data/jpeg-192x192/train/blanket flower',\n",
       " 'data/jpeg-192x192/train/common tulip',\n",
       " 'data/jpeg-192x192/train/giant white arum lily',\n",
       " 'data/jpeg-192x192/train/wild rose',\n",
       " 'data/jpeg-192x192/train/morning glory',\n",
       " 'data/jpeg-192x192/train/thorn apple',\n",
       " 'data/jpeg-192x192/train/pincushion flower',\n",
       " 'data/jpeg-192x192/train/tree mallow',\n",
       " 'data/jpeg-192x192/train/canna lily',\n",
       " 'data/jpeg-192x192/train/camellia',\n",
       " 'data/jpeg-192x192/train/pink-yellow dahlia',\n",
       " 'data/jpeg-192x192/train/bee balm',\n",
       " 'data/jpeg-192x192/train/wild geranium',\n",
       " 'data/jpeg-192x192/train/artichoke',\n",
       " 'data/jpeg-192x192/train/black-eyed susan',\n",
       " 'data/jpeg-192x192/train/ruby-lipped cattleya',\n",
       " 'data/jpeg-192x192/train/clematis',\n",
       " 'data/jpeg-192x192/train/prince of wales feathers',\n",
       " 'data/jpeg-192x192/train/hibiscus',\n",
       " 'data/jpeg-192x192/train/cautleya spicata',\n",
       " 'data/jpeg-192x192/train/lenten rose',\n",
       " 'data/jpeg-192x192/train/red ginger',\n",
       " \"data/jpeg-192x192/train/colt's foot\",\n",
       " 'data/jpeg-192x192/train/hippeastrum ',\n",
       " 'data/jpeg-192x192/train/mallow',\n",
       " 'data/jpeg-192x192/train/californian poppy',\n",
       " 'data/jpeg-192x192/train/corn poppy',\n",
       " 'data/jpeg-192x192/train/moon orchid',\n",
       " 'data/jpeg-192x192/train/passion flower',\n",
       " 'data/jpeg-192x192/train/grape hyacinth',\n",
       " 'data/jpeg-192x192/train/japanese anemone',\n",
       " 'data/jpeg-192x192/train/watercress',\n",
       " 'data/jpeg-192x192/train/cape flower',\n",
       " 'data/jpeg-192x192/train/osteospermum',\n",
       " 'data/jpeg-192x192/train/barberton daisy',\n",
       " 'data/jpeg-192x192/train/bougainvillea',\n",
       " 'data/jpeg-192x192/train/magnolia',\n",
       " 'data/jpeg-192x192/train/sunflower',\n",
       " 'data/jpeg-192x192/train/daffodil',\n",
       " 'data/jpeg-192x192/train/wallflower']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subfolder_list(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42af93",
   "metadata": {},
   "source": [
    "### introduce early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357c6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863710bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782406ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3117cd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9bdba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "116d9ae6",
   "metadata": {},
   "source": [
    "### increase batch size and epochs to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df2cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "#TEST_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with multiple hidden layers\n",
    "model_3 = models.Sequential()\n",
    "model_3.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model_3.add(layers.MaxPooling2D(pool_size=(2,2),\n",
    "                     strides=2))\n",
    "model_3.add(layers.Flatten())        \n",
    "# possibly more layers here\n",
    "model_3.add(layers.Dense(128))\n",
    "model_3.add(layers.Activation('relu'))\n",
    "model_3.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model_3.add(layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad34aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimizer, cost lost function, and scoring metric\n",
    "model_3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b922994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_3.fit(train_img, train_lb, epochs=100, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c02c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model_3.evaluate(val_img, val_lb, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde8974",
   "metadata": {},
   "source": [
    "### adding dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "#TEST_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with multiple hidden layers\n",
    "model_4 = models.Sequential()\n",
    "model_4.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model_4.add(layers.MaxPooling2D(pool_size=(2,2),\n",
    "                     strides=2))\n",
    "model_4.add(layers.Flatten())        \n",
    "# possibly more layers here\n",
    "model_4.add(layers.Dropout(0.2, input_shape=(IMAGE_DIMENSION,)))\n",
    "model_4.add(layers.Dense(128, activation='relu'))\n",
    "model_4.add(layers.Dense(64, activation='relu'))\n",
    "model_4.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model_4.add(layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b39e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b369d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimizer, cost lost function, and scoring metric\n",
    "model_4.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2d4d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_4.fit(train_img, train_lb, epochs=100, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5953679",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model_4.evaluate(val_img, val_lb, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a20d2",
   "metadata": {},
   "source": [
    "### shuffle is turned on. more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b9e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with multiple hidden layers\n",
    "model_1 = models.Sequential()\n",
    "model_1.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model_1.add(layers.BatchNormalization())\n",
    "model_1.add(layers.MaxPooling2D(pool_size=(2,2),\n",
    "                     strides=2))\n",
    "model_1.add(layers.Flatten())        \n",
    "# possibly more layers here\n",
    "model_1.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model_1.add(layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d93a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83816901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimizer, cost lost function, and scoring metric\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34aa2f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_1.fit(train_img, train_lb, epochs=150, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c35175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_loss, val_acc = model_1.evaluate(val_img, val_lb, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01900c",
   "metadata": {},
   "source": [
    "### without flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff33361",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    pretrained_model = tf.keras.applications.Xception(\n",
    "        weights='imagenet',\n",
    "        include_top=False ,\n",
    "        input_shape=[IMAGE_DIMENSION, IMAGE_DIMENSION, 3]\n",
    "    )\n",
    "    pretrained_model.trainable = False\n",
    "    \n",
    "    model1 = tf.keras.Sequential([\n",
    "        # To a base pretrained on ImageNet to extract features from images...\n",
    "        pretrained_model,\n",
    "        # ... attach a new head to act as a classifier.\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        # tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

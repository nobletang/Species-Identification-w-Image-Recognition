{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1593fbe0",
   "metadata": {},
   "source": [
    "Dataset organized kindly by user Mourad. https://www.kaggle.com/msheriey/104-flowers-garden-of-eden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57708f20",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d75b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfd567",
   "metadata": {},
   "source": [
    "# global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b60659f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIMENSION = 192\n",
    "VECTOR_LEN = IMAGE_DIMENSION**2\n",
    "NUM_CLASS = 104\n",
    "\n",
    "TRAIN_DIR = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/train'\n",
    "VAL_DIR = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/val'\n",
    "#TEST_DIR = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/test/test'\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 128\n",
    "#TEST_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989fb6c",
   "metadata": {},
   "source": [
    "# generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34c127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_dataset():\n",
    "    train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(TRAIN_DIR,\n",
    "                                                                             target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "                                                                             batch_size=TRAIN_BATCH_SIZE,\n",
    "                                                                             shuffle=False\n",
    "                                                                            )\n",
    "    \n",
    "    return next(train_generator)\n",
    "        \n",
    "def generate_val_dataset():\n",
    "    val_generator = ImageDataGenerator().flow_from_directory(VAL_DIR,\n",
    "                                                             target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "                                                             batch_size=VAL_BATCH_SIZE,\n",
    "                                                             shuffle=False\n",
    "                                                            )\n",
    "    \n",
    "    return next(val_generator)\n",
    "        \n",
    "def generate_test_dataset():\n",
    "    test_generator = ImageDataGenerator().flow_from_directory(TEST_DIR,\n",
    "                                                             target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "                                                             batch_size=TEST_BATCH_SIZE,\n",
    "                                                             shuffle=False\n",
    "                                                            )\n",
    "    \n",
    "    return next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68dae13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12753 images belonging to 104 classes.\n",
      "Found 3712 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_img, train_lb = generate_train_dataset()\n",
    "val_img, val_lb = generate_val_dataset()\n",
    "#test_img, test_lb = generate_test_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d44fe8",
   "metadata": {},
   "source": [
    "# explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29d5271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 192, 192, 3), (32, 104))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape, train_lb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b07afd",
   "metadata": {},
   "source": [
    "# reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23be7f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 192, 192, 3), (32, 104))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape, train_lb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4ede7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 192, 192, 3)\n",
      "(32, 104)\n",
      "(32, 192, 192, 3)\n",
      "(32, 104)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_img))\n",
    "print(np.shape(train_lb))\n",
    "print(np.shape(val_img))\n",
    "print(np.shape(val_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0abda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110592, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_unrow = train_img.reshape(TRAIN_BATCH_SIZE, -1).T\n",
    "np.shape(train_img_unrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85bd995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110592, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_img_unrow = val_img.reshape(VAL_BATCH_SIZE, -1).T\n",
    "np.shape(val_img_unrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790db33",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540eeaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b77bfca",
   "metadata": {},
   "source": [
    "# cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70c0aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with multiple hidden layers\n",
    "model_1 = models.Sequential()\n",
    "model_1.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model_1.add(layers.MaxPooling2D(pool_size=(2,2),\n",
    "                     strides=2))\n",
    "model_1.add(layers.Flatten())        \n",
    "# possible more layers here\n",
    "\n",
    "model_1.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model_1.add(layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72bf39bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 294912)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 104)               30670952  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 104)               0         \n",
      "=================================================================\n",
      "Total params: 30,671,368\n",
      "Trainable params: 30,671,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2485b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimizer, cost lost function, and scoring metric\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6b4d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x34f935040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x34f935040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 0.6871 - accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.1124 - accuracy: 0.5625\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0076 - accuracy: 0.9062\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0883 - accuracy: 0.4375\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0163 - accuracy: 0.5938\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0061 - accuracy: 0.9062\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0229 - accuracy: 0.8438\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0358 - accuracy: 0.7500\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0326 - accuracy: 0.7500\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0219 - accuracy: 0.8438\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0119 - accuracy: 0.9062\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0059 - accuracy: 0.9688\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0027 - accuracy: 0.9688\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 4.6496e-04 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0017 - accuracy: 0.9688\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0061 - accuracy: 0.8750\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0082 - accuracy: 0.8125\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0039 - accuracy: 0.9688\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0012 - accuracy: 0.9688\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 1.7231e-04 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 8.7075e-05 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 3.8024e-04 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 9.2175e-04 - accuracy: 0.9688\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0014 - accuracy: 0.9688\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0016 - accuracy: 0.9688\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0015 - accuracy: 0.9688\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0011 - accuracy: 0.9688\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 5.2707e-04 - accuracy: 0.9688\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 1.6310e-04 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 3.9204e-05 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 9.7555e-06 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 2.7640e-06 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 9.3815e-07 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 4.2938e-07 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 3.0820e-07 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 3.2572e-07 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 4.1022e-07 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 5.4027e-07 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 7.1053e-07 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 9.1954e-07 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 1.1672e-06 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 1.4523e-06 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 1.7725e-06 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 2.1235e-06 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 2.4994e-06 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 2.8927e-06 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 3.2953e-06 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 3.6967e-06 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 4.0877e-06 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 4.4570e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17b70c160>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(train_img, train_lb, epochs=50, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8d5d781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x338de11f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x338de11f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 1.0577 - accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model_1.evaluate(val_img, val_lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d2abc",
   "metadata": {},
   "source": [
    "Probably some overfitting in this cnn model. Will try adding more layers to this and see if that reduces overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3ed2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with multiple hidden layers\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model_2.add(layers.MaxPooling2D(pool_size=(2,2),\n",
    "                     strides=2))\n",
    "model_2.add(layers.Flatten())        \n",
    "# possible more layers here\n",
    "model_2.add(layers.Dense(128))\n",
    "model_2.add(layers.Activation('relu'))\n",
    "model_2.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model_2.add(layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad68d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 294912)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               37748864  \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 104)               13416     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 104)               0         \n",
      "=================================================================\n",
      "Total params: 37,762,696\n",
      "Trainable params: 37,762,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "710f1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimizer, cost lost function, and scoring metric\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3782cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x297f46af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x297f46af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 0.6904 - accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 2.4139 - accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 1.5871 - accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 1.3332 - accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 1.1505 - accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.6924 - accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.4275 - accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.3933 - accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.3067 - accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.1735 - accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0712 - accuracy: 0.9062\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0422 - accuracy: 0.9062\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0328 - accuracy: 0.9062\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0224 - accuracy: 0.9062\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0177 - accuracy: 0.8750\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0183 - accuracy: 0.9062\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0174 - accuracy: 0.9375\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0072 - accuracy: 0.9688\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.0047 - accuracy: 0.9688\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0033 - accuracy: 0.9688\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0030 - accuracy: 0.9688\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0032 - accuracy: 0.9688\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0035 - accuracy: 0.9688\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.0037 - accuracy: 0.9688\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0035 - accuracy: 0.9688\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0030 - accuracy: 0.9688\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0024 - accuracy: 0.9688\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0019 - accuracy: 0.9688\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 9.1366e-04 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 8.2122e-04 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 8.0847e-04 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 8.3632e-04 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 8.7273e-04 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 8.9580e-04 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 8.9409e-04 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 8.6571e-04 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 8.1560e-04 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 7.5315e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3261fda30>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(train_img, train_lb, epochs=50, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d2d9bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x35ec3c820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x35ec3c820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.6603 - accuracy: 0.7812\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model_2.evaluate(val_img, val_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fbf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

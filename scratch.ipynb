{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "005481c5",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4fa459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.applications import Xception, DenseNet201\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import random\n",
    "from PIL import Image\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c43376",
   "metadata": {},
   "source": [
    "# global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42247e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image size options: 192, 224, 311, 512\n",
    "IMAGE_DIMENSION = 192\n",
    "VECTOR_LEN = IMAGE_DIMENSION**2\n",
    "NUM_CLASS = 96\n",
    "\n",
    "train_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/train'\n",
    "val_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/val'\n",
    "test_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/test'\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b17c6a",
   "metadata": {},
   "source": [
    "# eda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b680258f",
   "metadata": {},
   "source": [
    "# generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b69672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ad8bd",
   "metadata": {},
   "source": [
    "### defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f20c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subfolder_list(source_folder):\n",
    "    # returns a list of all the subfolders in the source_folder\n",
    "    class_list_dir = []\n",
    "\n",
    "    for file in os.listdir(source_folder):\n",
    "        d = os.path.join(source_folder, file)\n",
    "        if os.path.isdir(d):\n",
    "            class_list_dir.append(d)\n",
    "\n",
    "    return class_list_dir\n",
    "\n",
    "def map_classes(subfolder_list):\n",
    "    # returns a dict with the flower as the key, and (count, directory) as the values\n",
    "    class_dict = {}\n",
    "\n",
    "    for class_folder in subfolder_list:\n",
    "        file_count = sum(len(files) for _, _, files in os.walk(class_folder))\n",
    "        class_dict[class_folder[24:]] = file_count, class_folder\n",
    "        \n",
    "    return class_dict\n",
    "\n",
    "def get_flower_count(class_dict):\n",
    "    # returns a list of the number of flowers found in the dict\n",
    "    flower_count = []\n",
    "\n",
    "    for flower in list(class_dict.values()):\n",
    "        flower_count.append(flower[0])\n",
    "\n",
    "    return flower_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b374f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(flower_count):\n",
    "    # returns basic metrics when given a list of flower value count\n",
    "    class_std = np.std(flower_count)\n",
    "    class_max = max(flower_count)\n",
    "    class_min = min(flower_count)\n",
    "    class_mean = np.mean(flower_count)\n",
    "    class_first_quartile = np.percentile(flower_count, 25)\n",
    "    class_third_quartile = np.percentile(flower_count, 75)\n",
    "    class_tenth_percentile = np.percentile(flower_count, 10)\n",
    "    class_fifth_percentile = np.percentile(flower_count, 5)\n",
    "\n",
    "    print(f'0. standard deviation: {class_std}')\n",
    "    print(f'1. max: {class_max}')\n",
    "    print(f'2. min: {class_min}')\n",
    "    print(f'3. mean: {class_mean}')\n",
    "    print(f'4. 25%: {class_first_quartile}')\n",
    "    print(f'5. 75%: {class_third_quartile}')\n",
    "    print(f'6. 10%: {class_tenth_percentile}')\n",
    "    print(f'7. 5%: {class_fifth_percentile}')\n",
    "    \n",
    "    return (class_std, class_max, class_min, class_mean, class_first_quartile,\n",
    "class_third_quartile, class_tenth_percentile, class_fifth_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b3799",
   "metadata": {},
   "source": [
    "### testing above functions / eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29f45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subfolders = get_subfolder_list(train_dir)\n",
    "train_dict = map_classes(train_subfolders)\n",
    "train_flower_count = get_flower_count(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d526bdaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/jpeg-192x192/train/toad lily',\n",
       " 'data/jpeg-192x192/train/love in the mist',\n",
       " 'data/jpeg-192x192/train/monkshood',\n",
       " 'data/jpeg-192x192/train/azalea',\n",
       " 'data/jpeg-192x192/train/fritillary']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subfolders[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0707d52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. standard deviation: 132.99130714962862\n",
      "1. max: 707\n",
      "2. min: 16\n",
      "3. mean: 111.1826923076923\n",
      "4. 25%: 30.5\n",
      "5. 75%: 113.5\n",
      "6. 10%: 19.0\n",
      "7. 5%: 17.15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(132.99130714962862, 707, 16, 111.1826923076923, 30.5, 113.5, 19.0, 17.15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(train_flower_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4789abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_subfolders = get_subfolder_list(val_dir)\n",
    "val_dict = map_classes(val_subfolders)\n",
    "val_flower_count = get_flower_count(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "740c2486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. standard deviation: 42.990624407913046\n",
      "1. max: 228\n",
      "2. min: 5\n",
      "3. mean: 35.69230769230769\n",
      "4. 25%: 9.0\n",
      "5. 75%: 37.0\n",
      "6. 10%: 6.0\n",
      "7. 5%: 6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42.990624407913046, 228, 5, 35.69230769230769, 9.0, 37.0, 6.0, 6.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(val_flower_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117a290",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf36e89a",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f3519f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_subfolder(source):\n",
    "    # copies source folder to a new directory with '_new' attached to the end of the folder name\n",
    "    prev_dir_index = source.rfind('/')\n",
    "    destination = source[:prev_dir_index] + '_new' + source[prev_dir_index:]\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(destination):\n",
    "        result = shutil.copytree(source, destination, symlinks=False, ignore=None, \n",
    "                                 copy_function=shutil.copy2, ignore_dangling_symlinks=False, \n",
    "                                 dirs_exist_ok=False)\n",
    "    else:\n",
    "        print(f'{destination} already exists')\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def item_count(folder):\n",
    "    # helper function to identify which folders to remove\n",
    "    file_count = sum(len(files) for _, _, files in os.walk(folder))\n",
    "    \n",
    "    return file_count\n",
    "    \n",
    "def get_short_list(subfolder_list, n):\n",
    "    # find a list of classes that are divided by the specified n value\n",
    "    temp_dict = map_classes(subfolder_list)\n",
    "    temp_flower_count = get_flower_count(temp_dict)\n",
    "    \n",
    "    move_list = []\n",
    "    ignore_list = []\n",
    "    \n",
    "    percent_cutoff = np.percentile(temp_flower_count, n)\n",
    "    \n",
    "    for subfolder in subfolder_list:\n",
    "        if item_count(subfolder) >= percent_cutoff:\n",
    "            move_list.append(subfolder)\n",
    "        else:\n",
    "            ignore_list.append(subfolder)\n",
    "            \n",
    "    return move_list, ignore_list\n",
    "    \n",
    "def trim(subfolder_list):\n",
    "    # main function used to copy classes into new train, val, test \n",
    "    for subfolder in subfolder_list:\n",
    "        copy_subfolder(subfolder)\n",
    "\n",
    "        val_subfolder = subfolder.replace('/train/', '/val/')\n",
    "        copy_subfolder(val_subfolder)\n",
    "\n",
    "        test_subfolder = subfolder.replace('/train/', '/test/')\n",
    "        copy_subfolder(test_subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24807f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_list, ignore_list = get_short_list(train_subfolders, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "406a8a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/jpeg-192x192/train/toad lily',\n",
       " 'data/jpeg-192x192/train/love in the mist',\n",
       " 'data/jpeg-192x192/train/monkshood',\n",
       " 'data/jpeg-192x192/train/azalea',\n",
       " 'data/jpeg-192x192/train/fritillary']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trim(move_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12433225",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/jpeg-192x192/train_new/iris\n",
      "23\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'data/jpeg-192x192/train_new/iris'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_16887/3544434631.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/jpeg-192x192/train/iris'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcopy_subfolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_16887/3606631599.py\u001b[0m in \u001b[0;36mcopy_subfolder\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_dir_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     result = shutil.copytree(source, destination, symlinks=False, ignore=None, \n\u001b[0m\u001b[1;32m      8\u001b[0m                                   copy_function=shutil.copy2, ignore_dangling_symlinks=False, dirs_exist_ok=False)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    558\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mignored_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirs_exist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0muse_srcentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'data/jpeg-192x192/train_new/iris'"
     ]
    }
   ],
   "source": [
    "src = 'data/jpeg-192x192/train/iris'\n",
    "\n",
    "copy_subfolder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9d1efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the directories as new folders are made \n",
    "train_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/train_new'\n",
    "val_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/val_new'\n",
    "test_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/test_new'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b6dad",
   "metadata": {},
   "source": [
    "# generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84289b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb8c6d",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acd6f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fea35e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2853265e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 94, 94, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 31, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 45632)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 45632)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1460256   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 96)                3168      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 1,482,336\n",
      "Trainable params: 1,482,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,4)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c51a8b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a9b73ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a9b73ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - ETA: 0s - loss: 4.1504 - accuracy: 0.0831WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2aa828430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2aa828430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - 93s 81ms/step - loss: 4.1502 - accuracy: 0.0831 - val_loss: 198.8625 - val_accuracy: 0.1309\n",
      "Epoch 2/20\n",
      "1133/1133 [==============================] - 91s 80ms/step - loss: 3.5431 - accuracy: 0.1414 - val_loss: 292.6904 - val_accuracy: 0.0914\n",
      "Epoch 3/20\n",
      "1133/1133 [==============================] - 91s 80ms/step - loss: 3.3680 - accuracy: 0.1529 - val_loss: 259.4226 - val_accuracy: 0.1064\n",
      "Epoch 4/20\n",
      "1133/1133 [==============================] - 92s 82ms/step - loss: 3.2811 - accuracy: 0.1679 - val_loss: 319.7148 - val_accuracy: 0.0840\n",
      "Epoch 5/20\n",
      "1133/1133 [==============================] - 92s 82ms/step - loss: 3.2323 - accuracy: 0.1699 - val_loss: 307.3159 - val_accuracy: 0.1410\n",
      "Epoch 6/20\n",
      "1133/1133 [==============================] - 110s 97ms/step - loss: 3.1598 - accuracy: 0.1898 - val_loss: 248.6897 - val_accuracy: 0.1266\n",
      "Epoch 7/20\n",
      "1133/1133 [==============================] - 92s 81ms/step - loss: 3.1714 - accuracy: 0.1904 - val_loss: 306.2546 - val_accuracy: 0.1195\n",
      "Epoch 8/20\n",
      "1133/1133 [==============================] - 91s 81ms/step - loss: 3.1158 - accuracy: 0.1925 - val_loss: 312.7307 - val_accuracy: 0.1364\n",
      "Epoch 9/20\n",
      "1133/1133 [==============================] - 91s 80ms/step - loss: 3.0977 - accuracy: 0.1951 - val_loss: 296.1233 - val_accuracy: 0.0998\n",
      "Epoch 10/20\n",
      "1133/1133 [==============================] - 92s 81ms/step - loss: 3.0882 - accuracy: 0.1978 - val_loss: 278.4246 - val_accuracy: 0.1176\n",
      "Epoch 11/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 3.0760 - accuracy: 0.1987 - val_loss: 339.5401 - val_accuracy: 0.1034\n",
      "Epoch 12/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 3.0500 - accuracy: 0.2083 - val_loss: 343.1902 - val_accuracy: 0.1315\n",
      "Epoch 13/20\n",
      "1133/1133 [==============================] - 92s 81ms/step - loss: 3.0039 - accuracy: 0.2182 - val_loss: 366.4948 - val_accuracy: 0.0990\n",
      "Epoch 14/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 2.9869 - accuracy: 0.2140 - val_loss: 327.6807 - val_accuracy: 0.1388\n",
      "Epoch 15/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 2.9877 - accuracy: 0.2205 - val_loss: 299.0831 - val_accuracy: 0.0846\n",
      "Epoch 16/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 2.9665 - accuracy: 0.2237 - val_loss: 288.3284 - val_accuracy: 0.1348\n",
      "Epoch 17/20\n",
      "1133/1133 [==============================] - 95s 84ms/step - loss: 2.9390 - accuracy: 0.2255 - val_loss: 317.9936 - val_accuracy: 0.0824\n",
      "Epoch 18/20\n",
      "1133/1133 [==============================] - 92s 81ms/step - loss: 2.9377 - accuracy: 0.2282 - val_loss: 245.2157 - val_accuracy: 0.1394\n",
      "Epoch 19/20\n",
      "1133/1133 [==============================] - 94s 83ms/step - loss: 2.9203 - accuracy: 0.2322 - val_loss: 280.0243 - val_accuracy: 0.1356\n",
      "Epoch 20/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 2.9073 - accuracy: 0.2345 - val_loss: 285.4686 - val_accuracy: 0.1252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a2ddc8b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3cd37",
   "metadata": {},
   "source": [
    "### simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2304c9c5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 192, 192, 64)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 96, 96, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 589824)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 96)                56623200  \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 56,624,032\n",
      "Trainable params: 56,624,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa8dd040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa8dd040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - ETA: 0s - loss: 2673393.6038 - accuracy: 0.0051WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x29fae6940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x29fae6940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - 223s 196ms/step - loss: 2678262.8259 - accuracy: 0.0051 - val_loss: 1087628509184.0000 - val_accuracy: 0.0060\n",
      "Epoch 2/20\n",
      "1133/1133 [==============================] - 221s 195ms/step - loss: 38240184.5996 - accuracy: 0.0051 - val_loss: 3938470854656.0000 - val_accuracy: 0.0046\n",
      "Epoch 3/20\n",
      " 241/1133 [=====>........................] - ETA: 2:37 - loss: 92540924.4149 - accuracy: 0.0024"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_17824/537911790.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# training begins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n\u001b[0m\u001b[1;32m     21\u001b[0m           validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=64, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55b8b4",
   "metadata": {},
   "source": [
    "### simple model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c04cad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 294912)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 96)                28311648  \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 28,312,064\n",
      "Trainable params: 28,312,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa91d9d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa91d9d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - ETA: 0s - loss: 1319731.7839 - accuracy: 0.0063WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a9b47a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a9b47a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - 117s 103ms/step - loss: 1322061.5310 - accuracy: 0.0063 - val_loss: 534368419840.0000 - val_accuracy: 0.0115\n",
      "Epoch 2/20\n",
      "1133/1133 [==============================] - 117s 103ms/step - loss: 18463254.4224 - accuracy: 0.0089 - val_loss: 1936699162624.0000 - val_accuracy: 0.0090\n",
      "Epoch 3/20\n",
      " 219/1133 [====>.........................] - ETA: 1:26 - loss: 44264457.0046 - accuracy: 0.0101"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_17824/2617984591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# training begins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n\u001b[0m\u001b[1;32m     21\u001b[0m           validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1903784",
   "metadata": {},
   "source": [
    "### simple 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a569b409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 95, 95, 16)        2064      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 47, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 35344)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 96)                3393120   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 3,395,600\n",
      "Trainable params: 3,395,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a0783550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a0783550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - ETA: 0s - loss: 3.7146 - accuracy: 0.1315WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a0784790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a0784790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 3.7144 - accuracy: 0.1316 - val_loss: 439.0936 - val_accuracy: 0.1770\n",
      "Epoch 2/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 3.0447 - accuracy: 0.2211 - val_loss: 603.0725 - val_accuracy: 0.1410\n",
      "Epoch 3/20\n",
      "1133/1133 [==============================] - 75s 66ms/step - loss: 2.8612 - accuracy: 0.2615 - val_loss: 443.6947 - val_accuracy: 0.2016\n",
      "Epoch 4/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.7252 - accuracy: 0.2865 - val_loss: 401.4994 - val_accuracy: 0.2054\n",
      "Epoch 5/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.6325 - accuracy: 0.3085 - val_loss: 494.7075 - val_accuracy: 0.2100\n",
      "Epoch 6/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.5958 - accuracy: 0.3216 - val_loss: 501.8578 - val_accuracy: 0.1819\n",
      "Epoch 7/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.5137 - accuracy: 0.3392 - val_loss: 462.8085 - val_accuracy: 0.2040\n",
      "Epoch 8/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.4689 - accuracy: 0.3534 - val_loss: 398.3182 - val_accuracy: 0.2387\n",
      "Epoch 9/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.4292 - accuracy: 0.3581 - val_loss: 922.8542 - val_accuracy: 0.1424\n",
      "Epoch 10/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.3890 - accuracy: 0.3693 - val_loss: 453.7575 - val_accuracy: 0.2313\n",
      "Epoch 11/20\n",
      "1133/1133 [==============================] - 76s 67ms/step - loss: 2.3238 - accuracy: 0.3921 - val_loss: 476.9605 - val_accuracy: 0.2182\n",
      "Epoch 12/20\n",
      "1133/1133 [==============================] - 73s 65ms/step - loss: 2.2791 - accuracy: 0.3935 - val_loss: 434.3531 - val_accuracy: 0.2111\n",
      "Epoch 13/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.2244 - accuracy: 0.4119 - val_loss: 405.6454 - val_accuracy: 0.2300\n",
      "Epoch 14/20\n",
      "1133/1133 [==============================] - 73s 65ms/step - loss: 2.1695 - accuracy: 0.4225 - val_loss: 369.4056 - val_accuracy: 0.2542\n",
      "Epoch 15/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.1629 - accuracy: 0.4242 - val_loss: 326.6906 - val_accuracy: 0.1923\n",
      "Epoch 16/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.1579 - accuracy: 0.4252 - val_loss: 357.5777 - val_accuracy: 0.2174\n",
      "Epoch 17/20\n",
      "1133/1133 [==============================] - 73s 65ms/step - loss: 2.1136 - accuracy: 0.4342 - val_loss: 372.6766 - val_accuracy: 0.2169\n",
      "Epoch 18/20\n",
      "1133/1133 [==============================] - 73s 65ms/step - loss: 2.0626 - accuracy: 0.4425 - val_loss: 383.1561 - val_accuracy: 0.2272\n",
      "Epoch 19/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.0743 - accuracy: 0.4483 - val_loss: 381.9819 - val_accuracy: 0.2057\n",
      "Epoch 20/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.0418 - accuracy: 0.4547 - val_loss: 362.4312 - val_accuracy: 0.2163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a07801f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (2,2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09a6e0d",
   "metadata": {},
   "source": [
    "### simple 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27ab94fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 96)                1476192   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 1,481,232\n",
      "Trainable params: 1,481,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa91dee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa91dee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - ETA: 0s - loss: 3.8757 - accuracy: 0.1023WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x29faf4e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x29faf4e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 3.8755 - accuracy: 0.1023 - val_loss: 422.0252 - val_accuracy: 0.1609\n",
      "Epoch 2/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 3.2827 - accuracy: 0.1850 - val_loss: 493.7076 - val_accuracy: 0.1781\n",
      "Epoch 3/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 3.0764 - accuracy: 0.2242 - val_loss: 492.5594 - val_accuracy: 0.1765\n",
      "Epoch 4/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.9393 - accuracy: 0.2506 - val_loss: 453.8977 - val_accuracy: 0.1983\n",
      "Epoch 5/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.7710 - accuracy: 0.2957 - val_loss: 408.0635 - val_accuracy: 0.1983\n",
      "Epoch 6/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.6936 - accuracy: 0.3045 - val_loss: 408.9839 - val_accuracy: 0.2147\n",
      "Epoch 7/20\n",
      "1133/1133 [==============================] - 80s 70ms/step - loss: 2.5913 - accuracy: 0.3302 - val_loss: 366.1572 - val_accuracy: 0.2294\n",
      "Epoch 8/20\n",
      "1133/1133 [==============================] - 77s 68ms/step - loss: 2.5156 - accuracy: 0.3508 - val_loss: 281.0432 - val_accuracy: 0.2226\n",
      "Epoch 9/20\n",
      "1133/1133 [==============================] - 77s 68ms/step - loss: 2.4444 - accuracy: 0.3641 - val_loss: 423.5385 - val_accuracy: 0.1899\n",
      "Epoch 10/20\n",
      "1133/1133 [==============================] - 79s 70ms/step - loss: 2.3870 - accuracy: 0.3714 - val_loss: 341.9017 - val_accuracy: 0.2103\n",
      "Epoch 11/20\n",
      "1133/1133 [==============================] - 76s 67ms/step - loss: 2.3317 - accuracy: 0.3850 - val_loss: 313.8206 - val_accuracy: 0.2319\n",
      "Epoch 12/20\n",
      "1133/1133 [==============================] - 75s 66ms/step - loss: 2.3062 - accuracy: 0.3947 - val_loss: 454.9861 - val_accuracy: 0.1667\n",
      "Epoch 13/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.2406 - accuracy: 0.4033 - val_loss: 323.9634 - val_accuracy: 0.2002\n",
      "Epoch 14/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.2046 - accuracy: 0.4166 - val_loss: 377.5366 - val_accuracy: 0.1669\n",
      "Epoch 15/20\n",
      "1133/1133 [==============================] - 73s 65ms/step - loss: 2.1637 - accuracy: 0.4271 - val_loss: 411.3255 - val_accuracy: 0.1596\n",
      "Epoch 16/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.1476 - accuracy: 0.4366 - val_loss: 348.7963 - val_accuracy: 0.2106\n",
      "Epoch 17/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.1439 - accuracy: 0.4389 - val_loss: 329.3263 - val_accuracy: 0.2062\n",
      "Epoch 18/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.0731 - accuracy: 0.4484 - val_loss: 347.4140 - val_accuracy: 0.1833\n",
      "Epoch 19/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.0930 - accuracy: 0.4446 - val_loss: 340.5002 - val_accuracy: 0.2310\n",
      "Epoch 20/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.0479 - accuracy: 0.4554 - val_loss: 402.1681 - val_accuracy: 0.1806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29fd6c370>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca839ac",
   "metadata": {},
   "source": [
    "### simple 4 - batch size increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c82b1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd5ad186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5654c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 96)                1476192   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 1,481,232\n",
      "Trainable params: 1,481,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a2da09d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a2da09d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 3.8327 - accuracy: 0.1131WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a7fe9820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a7fe9820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 72s 404ms/step - loss: 3.8311 - accuracy: 0.1133 - val_loss: 369.5526 - val_accuracy: 0.1792\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 3.1020 - accuracy: 0.2144 - val_loss: 396.5790 - val_accuracy: 0.1809\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.9497 - accuracy: 0.2413 - val_loss: 408.8349 - val_accuracy: 0.2019\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.8293 - accuracy: 0.2690 - val_loss: 410.4345 - val_accuracy: 0.1901\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.7180 - accuracy: 0.2877 - val_loss: 385.1939 - val_accuracy: 0.2220\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.6793 - accuracy: 0.3016 - val_loss: 422.7862 - val_accuracy: 0.2319\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.5712 - accuracy: 0.3273 - val_loss: 377.4276 - val_accuracy: 0.2471\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.4852 - accuracy: 0.3577 - val_loss: 387.7034 - val_accuracy: 0.2499\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.4794 - accuracy: 0.3468 - val_loss: 393.4761 - val_accuracy: 0.2867\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.3624 - accuracy: 0.3698 - val_loss: 373.3439 - val_accuracy: 0.2908\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.3172 - accuracy: 0.3972 - val_loss: 377.1596 - val_accuracy: 0.2902\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 72s 406ms/step - loss: 2.2677 - accuracy: 0.4053 - val_loss: 412.6518 - val_accuracy: 0.2801\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 72s 402ms/step - loss: 2.2258 - accuracy: 0.4027 - val_loss: 398.9681 - val_accuracy: 0.2845\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 71s 400ms/step - loss: 2.1689 - accuracy: 0.4303 - val_loss: 421.6128 - val_accuracy: 0.2684\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.1416 - accuracy: 0.4334 - val_loss: 400.7722 - val_accuracy: 0.2938\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.1391 - accuracy: 0.4329 - val_loss: 409.9650 - val_accuracy: 0.2990\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.0973 - accuracy: 0.4417 - val_loss: 432.7876 - val_accuracy: 0.2916\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.0684 - accuracy: 0.4534 - val_loss: 426.4082 - val_accuracy: 0.2894\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 71s 398ms/step - loss: 2.0508 - accuracy: 0.4574 - val_loss: 383.3103 - val_accuracy: 0.3022\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.0311 - accuracy: 0.4711 - val_loss: 394.7541 - val_accuracy: 0.3096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a2dae700>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748b39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

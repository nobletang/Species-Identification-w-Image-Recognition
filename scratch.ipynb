{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d339242",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5780f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.applications import Xception, DenseNet201\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import random\n",
    "from PIL import Image\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5066e8f",
   "metadata": {},
   "source": [
    "# global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cff8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image size options: 192, 224, 311, 512\n",
    "IMAGE_DIMENSION = 192\n",
    "VECTOR_LEN = IMAGE_DIMENSION**2\n",
    "NUM_CLASS = 96\n",
    "\n",
    "train_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/train'\n",
    "val_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/val'\n",
    "test_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/test'\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f418dd2",
   "metadata": {},
   "source": [
    "# eda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a5eed",
   "metadata": {},
   "source": [
    "# generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c6fd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11459 images belonging to 104 classes.\n",
      "Found 3710 images belonging to 104 classes.\n",
      "Found 1294 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c8f6c",
   "metadata": {},
   "source": [
    "### defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8491143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subfolder_list(source_folder):\n",
    "    # returns a list of all the subfolders in the source_folder\n",
    "    class_list_dir = []\n",
    "\n",
    "    for file in os.listdir(source_folder):\n",
    "        d = os.path.join(source_folder, file)\n",
    "        if os.path.isdir(d):\n",
    "            class_list_dir.append(d)\n",
    "\n",
    "    return class_list_dir\n",
    "\n",
    "def map_classes(subfolder_list):\n",
    "    # returns a dict with the flower as the key, and (count, directory) as the values\n",
    "    class_dict = {}\n",
    "\n",
    "    for class_folder in subfolder_list:\n",
    "        file_count = sum(len(files) for _, _, files in os.walk(class_folder))\n",
    "        class_dict[class_folder[24:]] = file_count, class_folder\n",
    "        \n",
    "    return class_dict\n",
    "\n",
    "def get_flower_count(class_dict):\n",
    "    # returns a list of the number of flowers found in the dict\n",
    "    flower_count = []\n",
    "\n",
    "    for flower in list(class_dict.values()):\n",
    "        flower_count.append(flower[0])\n",
    "\n",
    "    return flower_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab662cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(flower_count):\n",
    "    # returns basic metrics when given a list of flower value count\n",
    "    class_std = np.std(flower_count)\n",
    "    class_max = max(flower_count)\n",
    "    class_min = min(flower_count)\n",
    "    class_mean = np.mean(flower_count)\n",
    "    class_first_quartile = np.percentile(flower_count, 25)\n",
    "    class_third_quartile = np.percentile(flower_count, 75)\n",
    "    class_tenth_percentile = np.percentile(flower_count, 10)\n",
    "    class_fifth_percentile = np.percentile(flower_count, 5)\n",
    "\n",
    "    print(f'0. standard deviation: {class_std}')\n",
    "    print(f'1. max: {class_max}')\n",
    "    print(f'2. min: {class_min}')\n",
    "    print(f'3. mean: {class_mean}')\n",
    "    print(f'4. 25%: {class_first_quartile}')\n",
    "    print(f'5. 75%: {class_third_quartile}')\n",
    "    print(f'6. 10%: {class_tenth_percentile}')\n",
    "    print(f'7. 5%: {class_fifth_percentile}')\n",
    "    \n",
    "    return (class_std, class_max, class_min, class_mean, class_first_quartile,\n",
    "class_third_quartile, class_tenth_percentile, class_fifth_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab42d2",
   "metadata": {},
   "source": [
    "### testing above functions / eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a795b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subfolders = get_subfolder_list(train_dir)\n",
    "train_dict = map_classes(train_subfolders)\n",
    "train_flower_count = get_flower_count(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf9a86c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/jpeg-192x192/train/toad lily',\n",
       " 'data/jpeg-192x192/train/love in the mist',\n",
       " 'data/jpeg-192x192/train/monkshood',\n",
       " 'data/jpeg-192x192/train/azalea',\n",
       " 'data/jpeg-192x192/train/fritillary']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subfolders[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e5ea534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. standard deviation: 132.99130714962862\n",
      "1. max: 707\n",
      "2. min: 16\n",
      "3. mean: 111.1826923076923\n",
      "4. 25%: 30.5\n",
      "5. 75%: 113.5\n",
      "6. 10%: 19.0\n",
      "7. 5%: 17.15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(132.99130714962862, 707, 16, 111.1826923076923, 30.5, 113.5, 19.0, 17.15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(train_flower_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9daf34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_subfolders = get_subfolder_list(val_dir)\n",
    "val_dict = map_classes(val_subfolders)\n",
    "val_flower_count = get_flower_count(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1efd13c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. standard deviation: 42.990624407913046\n",
      "1. max: 228\n",
      "2. min: 5\n",
      "3. mean: 35.69230769230769\n",
      "4. 25%: 9.0\n",
      "5. 75%: 37.0\n",
      "6. 10%: 6.0\n",
      "7. 5%: 6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42.990624407913046, 228, 5, 35.69230769230769, 9.0, 37.0, 6.0, 6.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(val_flower_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844269b",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25a06f",
   "metadata": {},
   "source": [
    "### more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5feda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_subfolder(source):\n",
    "    # copies source folder to a new directory with '_new' attached to the end of the folder name\n",
    "    prev_dir_index = source.rfind('/')\n",
    "    destination = source[:prev_dir_index] + '_new' + source[prev_dir_index:]\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(destination):\n",
    "        result = shutil.copytree(source, destination, symlinks=False, ignore=None, \n",
    "                                 copy_function=shutil.copy2, ignore_dangling_symlinks=False, \n",
    "                                 dirs_exist_ok=False)\n",
    "    else:\n",
    "        print(f'{destination} already exists')\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def item_count(folder):\n",
    "    # helper function to identify which folders to remove\n",
    "    file_count = sum(len(files) for _, _, files in os.walk(folder))\n",
    "    \n",
    "    return file_count\n",
    "    \n",
    "def get_short_list(subfolder_list, n):\n",
    "    # find a list of classes that are divided by the specified n value\n",
    "    temp_dict = map_classes(subfolder_list)\n",
    "    temp_flower_count = get_flower_count(temp_dict)\n",
    "    \n",
    "    move_list = []\n",
    "    ignore_list = []\n",
    "    \n",
    "    percent_cutoff = np.percentile(temp_flower_count, n)\n",
    "    \n",
    "    for subfolder in subfolder_list:\n",
    "        if item_count(subfolder) >= percent_cutoff:\n",
    "            move_list.append(subfolder)\n",
    "        else:\n",
    "            ignore_list.append(subfolder)\n",
    "            \n",
    "    return move_list, ignore_list\n",
    "    \n",
    "def trim(subfolder_list):\n",
    "    # main function used to copy classes into new train, val, test \n",
    "    for subfolder in subfolder_list:\n",
    "        copy_subfolder(subfolder)\n",
    "\n",
    "        val_subfolder = subfolder.replace('/train/', '/val/')\n",
    "        copy_subfolder(val_subfolder)\n",
    "\n",
    "        test_subfolder = subfolder.replace('/train/', '/test/')\n",
    "        copy_subfolder(test_subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e520cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_list, ignore_list = get_short_list(train_subfolders, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43a5b276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/jpeg-192x192/train/toad lily',\n",
       " 'data/jpeg-192x192/train/love in the mist',\n",
       " 'data/jpeg-192x192/train/monkshood',\n",
       " 'data/jpeg-192x192/train/azalea',\n",
       " 'data/jpeg-192x192/train/fritillary']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0356d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "trim(move_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b93ff486",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/jpeg-192x192/train_new/iris\n",
      "23\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'data/jpeg-192x192/train_new/iris'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_16887/3544434631.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/jpeg-192x192/train/iris'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcopy_subfolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_16887/3606631599.py\u001b[0m in \u001b[0;36mcopy_subfolder\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_dir_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     result = shutil.copytree(source, destination, symlinks=False, ignore=None, \n\u001b[0m\u001b[1;32m      8\u001b[0m                                   copy_function=shutil.copy2, ignore_dangling_symlinks=False, dirs_exist_ok=False)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    558\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mignored_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirs_exist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0muse_srcentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'data/jpeg-192x192/train_new/iris'"
     ]
    }
   ],
   "source": [
    "src = 'data/jpeg-192x192/train/iris'\n",
    "\n",
    "copy_subfolder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b31083c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the directories as new folders are made \n",
    "train_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/train_new'\n",
    "val_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/val_new'\n",
    "test_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/test_new'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc9e583",
   "metadata": {},
   "source": [
    "# generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a51c983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3cddca",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56d3b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa17011d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39066e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 94, 94, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 31, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 45632)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 45632)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1460256   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 96)                3168      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 1,482,336\n",
      "Trainable params: 1,482,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        strides=(1,1),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,4)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5, input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8e3e706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a9b73ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a9b73ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - ETA: 0s - loss: 4.1504 - accuracy: 0.0831WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2aa828430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2aa828430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - 93s 81ms/step - loss: 4.1502 - accuracy: 0.0831 - val_loss: 198.8625 - val_accuracy: 0.1309\n",
      "Epoch 2/20\n",
      "1133/1133 [==============================] - 91s 80ms/step - loss: 3.5431 - accuracy: 0.1414 - val_loss: 292.6904 - val_accuracy: 0.0914\n",
      "Epoch 3/20\n",
      "1133/1133 [==============================] - 91s 80ms/step - loss: 3.3680 - accuracy: 0.1529 - val_loss: 259.4226 - val_accuracy: 0.1064\n",
      "Epoch 4/20\n",
      "1133/1133 [==============================] - 92s 82ms/step - loss: 3.2811 - accuracy: 0.1679 - val_loss: 319.7148 - val_accuracy: 0.0840\n",
      "Epoch 5/20\n",
      "1133/1133 [==============================] - 92s 82ms/step - loss: 3.2323 - accuracy: 0.1699 - val_loss: 307.3159 - val_accuracy: 0.1410\n",
      "Epoch 6/20\n",
      "1133/1133 [==============================] - 110s 97ms/step - loss: 3.1598 - accuracy: 0.1898 - val_loss: 248.6897 - val_accuracy: 0.1266\n",
      "Epoch 7/20\n",
      "1133/1133 [==============================] - 92s 81ms/step - loss: 3.1714 - accuracy: 0.1904 - val_loss: 306.2546 - val_accuracy: 0.1195\n",
      "Epoch 8/20\n",
      "1133/1133 [==============================] - 91s 81ms/step - loss: 3.1158 - accuracy: 0.1925 - val_loss: 312.7307 - val_accuracy: 0.1364\n",
      "Epoch 9/20\n",
      "1133/1133 [==============================] - 91s 80ms/step - loss: 3.0977 - accuracy: 0.1951 - val_loss: 296.1233 - val_accuracy: 0.0998\n",
      "Epoch 10/20\n",
      "1133/1133 [==============================] - 92s 81ms/step - loss: 3.0882 - accuracy: 0.1978 - val_loss: 278.4246 - val_accuracy: 0.1176\n",
      "Epoch 11/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 3.0760 - accuracy: 0.1987 - val_loss: 339.5401 - val_accuracy: 0.1034\n",
      "Epoch 12/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 3.0500 - accuracy: 0.2083 - val_loss: 343.1902 - val_accuracy: 0.1315\n",
      "Epoch 13/20\n",
      "1133/1133 [==============================] - 92s 81ms/step - loss: 3.0039 - accuracy: 0.2182 - val_loss: 366.4948 - val_accuracy: 0.0990\n",
      "Epoch 14/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 2.9869 - accuracy: 0.2140 - val_loss: 327.6807 - val_accuracy: 0.1388\n",
      "Epoch 15/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 2.9877 - accuracy: 0.2205 - val_loss: 299.0831 - val_accuracy: 0.0846\n",
      "Epoch 16/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 2.9665 - accuracy: 0.2237 - val_loss: 288.3284 - val_accuracy: 0.1348\n",
      "Epoch 17/20\n",
      "1133/1133 [==============================] - 95s 84ms/step - loss: 2.9390 - accuracy: 0.2255 - val_loss: 317.9936 - val_accuracy: 0.0824\n",
      "Epoch 18/20\n",
      "1133/1133 [==============================] - 92s 81ms/step - loss: 2.9377 - accuracy: 0.2282 - val_loss: 245.2157 - val_accuracy: 0.1394\n",
      "Epoch 19/20\n",
      "1133/1133 [==============================] - 94s 83ms/step - loss: 2.9203 - accuracy: 0.2322 - val_loss: 280.0243 - val_accuracy: 0.1356\n",
      "Epoch 20/20\n",
      "1133/1133 [==============================] - 93s 82ms/step - loss: 2.9073 - accuracy: 0.2345 - val_loss: 285.4686 - val_accuracy: 0.1252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a2ddc8b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa948a",
   "metadata": {},
   "source": [
    "### simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6464cfad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 192, 192, 64)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 96, 96, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 589824)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 96)                56623200  \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 56,624,032\n",
      "Trainable params: 56,624,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa8dd040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa8dd040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - ETA: 0s - loss: 2673393.6038 - accuracy: 0.0051WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x29fae6940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x29fae6940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - 223s 196ms/step - loss: 2678262.8259 - accuracy: 0.0051 - val_loss: 1087628509184.0000 - val_accuracy: 0.0060\n",
      "Epoch 2/20\n",
      "1133/1133 [==============================] - 221s 195ms/step - loss: 38240184.5996 - accuracy: 0.0051 - val_loss: 3938470854656.0000 - val_accuracy: 0.0046\n",
      "Epoch 3/20\n",
      " 241/1133 [=====>........................] - ETA: 2:37 - loss: 92540924.4149 - accuracy: 0.0024"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_17824/537911790.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# training begins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n\u001b[0m\u001b[1;32m     21\u001b[0m           validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=64, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3caa5d8",
   "metadata": {},
   "source": [
    "### simple model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c74daa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 294912)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 96)                28311648  \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 28,312,064\n",
      "Trainable params: 28,312,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa91d9d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa91d9d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - ETA: 0s - loss: 1319731.7839 - accuracy: 0.0063WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a9b47a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a9b47a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - 117s 103ms/step - loss: 1322061.5310 - accuracy: 0.0063 - val_loss: 534368419840.0000 - val_accuracy: 0.0115\n",
      "Epoch 2/20\n",
      "1133/1133 [==============================] - 117s 103ms/step - loss: 18463254.4224 - accuracy: 0.0089 - val_loss: 1936699162624.0000 - val_accuracy: 0.0090\n",
      "Epoch 3/20\n",
      " 219/1133 [====>.........................] - ETA: 1:26 - loss: 44264457.0046 - accuracy: 0.0101"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/_3l8mb_s4j967nwd3wg1v2d40000gn/T/ipykernel_17824/2617984591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# training begins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n\u001b[0m\u001b[1;32m     21\u001b[0m           validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/apple_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f1685",
   "metadata": {},
   "source": [
    "### simple 2 - another layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc27ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 95, 95, 16)        2064      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 47, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 35344)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 96)                3393120   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 3,395,600\n",
      "Trainable params: 3,395,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a0783550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a0783550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - ETA: 0s - loss: 3.7146 - accuracy: 0.1315WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a0784790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a0784790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 3.7144 - accuracy: 0.1316 - val_loss: 439.0936 - val_accuracy: 0.1770\n",
      "Epoch 2/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 3.0447 - accuracy: 0.2211 - val_loss: 603.0725 - val_accuracy: 0.1410\n",
      "Epoch 3/20\n",
      "1133/1133 [==============================] - 75s 66ms/step - loss: 2.8612 - accuracy: 0.2615 - val_loss: 443.6947 - val_accuracy: 0.2016\n",
      "Epoch 4/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.7252 - accuracy: 0.2865 - val_loss: 401.4994 - val_accuracy: 0.2054\n",
      "Epoch 5/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.6325 - accuracy: 0.3085 - val_loss: 494.7075 - val_accuracy: 0.2100\n",
      "Epoch 6/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.5958 - accuracy: 0.3216 - val_loss: 501.8578 - val_accuracy: 0.1819\n",
      "Epoch 7/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.5137 - accuracy: 0.3392 - val_loss: 462.8085 - val_accuracy: 0.2040\n",
      "Epoch 8/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.4689 - accuracy: 0.3534 - val_loss: 398.3182 - val_accuracy: 0.2387\n",
      "Epoch 9/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.4292 - accuracy: 0.3581 - val_loss: 922.8542 - val_accuracy: 0.1424\n",
      "Epoch 10/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.3890 - accuracy: 0.3693 - val_loss: 453.7575 - val_accuracy: 0.2313\n",
      "Epoch 11/20\n",
      "1133/1133 [==============================] - 76s 67ms/step - loss: 2.3238 - accuracy: 0.3921 - val_loss: 476.9605 - val_accuracy: 0.2182\n",
      "Epoch 12/20\n",
      "1133/1133 [==============================] - 73s 65ms/step - loss: 2.2791 - accuracy: 0.3935 - val_loss: 434.3531 - val_accuracy: 0.2111\n",
      "Epoch 13/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.2244 - accuracy: 0.4119 - val_loss: 405.6454 - val_accuracy: 0.2300\n",
      "Epoch 14/20\n",
      "1133/1133 [==============================] - 73s 65ms/step - loss: 2.1695 - accuracy: 0.4225 - val_loss: 369.4056 - val_accuracy: 0.2542\n",
      "Epoch 15/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.1629 - accuracy: 0.4242 - val_loss: 326.6906 - val_accuracy: 0.1923\n",
      "Epoch 16/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.1579 - accuracy: 0.4252 - val_loss: 357.5777 - val_accuracy: 0.2174\n",
      "Epoch 17/20\n",
      "1133/1133 [==============================] - 73s 65ms/step - loss: 2.1136 - accuracy: 0.4342 - val_loss: 372.6766 - val_accuracy: 0.2169\n",
      "Epoch 18/20\n",
      "1133/1133 [==============================] - 73s 65ms/step - loss: 2.0626 - accuracy: 0.4425 - val_loss: 383.1561 - val_accuracy: 0.2272\n",
      "Epoch 19/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.0743 - accuracy: 0.4483 - val_loss: 381.9819 - val_accuracy: 0.2057\n",
      "Epoch 20/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.0418 - accuracy: 0.4547 - val_loss: 362.4312 - val_accuracy: 0.2163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a07801f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (2,2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a16a2",
   "metadata": {},
   "source": [
    "### simple 3 - change the kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88be5837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 96)                1476192   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 1,481,232\n",
      "Trainable params: 1,481,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa91dee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa91dee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - ETA: 0s - loss: 3.8757 - accuracy: 0.1023WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x29faf4e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x29faf4e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 3.8755 - accuracy: 0.1023 - val_loss: 422.0252 - val_accuracy: 0.1609\n",
      "Epoch 2/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 3.2827 - accuracy: 0.1850 - val_loss: 493.7076 - val_accuracy: 0.1781\n",
      "Epoch 3/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 3.0764 - accuracy: 0.2242 - val_loss: 492.5594 - val_accuracy: 0.1765\n",
      "Epoch 4/20\n",
      "1133/1133 [==============================] - 73s 64ms/step - loss: 2.9393 - accuracy: 0.2506 - val_loss: 453.8977 - val_accuracy: 0.1983\n",
      "Epoch 5/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.7710 - accuracy: 0.2957 - val_loss: 408.0635 - val_accuracy: 0.1983\n",
      "Epoch 6/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.6936 - accuracy: 0.3045 - val_loss: 408.9839 - val_accuracy: 0.2147\n",
      "Epoch 7/20\n",
      "1133/1133 [==============================] - 80s 70ms/step - loss: 2.5913 - accuracy: 0.3302 - val_loss: 366.1572 - val_accuracy: 0.2294\n",
      "Epoch 8/20\n",
      "1133/1133 [==============================] - 77s 68ms/step - loss: 2.5156 - accuracy: 0.3508 - val_loss: 281.0432 - val_accuracy: 0.2226\n",
      "Epoch 9/20\n",
      "1133/1133 [==============================] - 77s 68ms/step - loss: 2.4444 - accuracy: 0.3641 - val_loss: 423.5385 - val_accuracy: 0.1899\n",
      "Epoch 10/20\n",
      "1133/1133 [==============================] - 79s 70ms/step - loss: 2.3870 - accuracy: 0.3714 - val_loss: 341.9017 - val_accuracy: 0.2103\n",
      "Epoch 11/20\n",
      "1133/1133 [==============================] - 76s 67ms/step - loss: 2.3317 - accuracy: 0.3850 - val_loss: 313.8206 - val_accuracy: 0.2319\n",
      "Epoch 12/20\n",
      "1133/1133 [==============================] - 75s 66ms/step - loss: 2.3062 - accuracy: 0.3947 - val_loss: 454.9861 - val_accuracy: 0.1667\n",
      "Epoch 13/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.2406 - accuracy: 0.4033 - val_loss: 323.9634 - val_accuracy: 0.2002\n",
      "Epoch 14/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.2046 - accuracy: 0.4166 - val_loss: 377.5366 - val_accuracy: 0.1669\n",
      "Epoch 15/20\n",
      "1133/1133 [==============================] - 73s 65ms/step - loss: 2.1637 - accuracy: 0.4271 - val_loss: 411.3255 - val_accuracy: 0.1596\n",
      "Epoch 16/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.1476 - accuracy: 0.4366 - val_loss: 348.7963 - val_accuracy: 0.2106\n",
      "Epoch 17/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.1439 - accuracy: 0.4389 - val_loss: 329.3263 - val_accuracy: 0.2062\n",
      "Epoch 18/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.0731 - accuracy: 0.4484 - val_loss: 347.4140 - val_accuracy: 0.1833\n",
      "Epoch 19/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.0930 - accuracy: 0.4446 - val_loss: 340.5002 - val_accuracy: 0.2310\n",
      "Epoch 20/20\n",
      "1133/1133 [==============================] - 74s 65ms/step - loss: 2.0479 - accuracy: 0.4554 - val_loss: 402.1681 - val_accuracy: 0.1806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29fd6c370>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e58a53",
   "metadata": {},
   "source": [
    "### simple 4 - batch size increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d24f2118",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "507e4899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6f8d90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 96)                1476192   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 1,481,232\n",
      "Trainable params: 1,481,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a2da09d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a2da09d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 3.8327 - accuracy: 0.1131WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a7fe9820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a7fe9820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 72s 404ms/step - loss: 3.8311 - accuracy: 0.1133 - val_loss: 369.5526 - val_accuracy: 0.1792\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 3.1020 - accuracy: 0.2144 - val_loss: 396.5790 - val_accuracy: 0.1809\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.9497 - accuracy: 0.2413 - val_loss: 408.8349 - val_accuracy: 0.2019\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.8293 - accuracy: 0.2690 - val_loss: 410.4345 - val_accuracy: 0.1901\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.7180 - accuracy: 0.2877 - val_loss: 385.1939 - val_accuracy: 0.2220\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.6793 - accuracy: 0.3016 - val_loss: 422.7862 - val_accuracy: 0.2319\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.5712 - accuracy: 0.3273 - val_loss: 377.4276 - val_accuracy: 0.2471\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.4852 - accuracy: 0.3577 - val_loss: 387.7034 - val_accuracy: 0.2499\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.4794 - accuracy: 0.3468 - val_loss: 393.4761 - val_accuracy: 0.2867\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.3624 - accuracy: 0.3698 - val_loss: 373.3439 - val_accuracy: 0.2908\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.3172 - accuracy: 0.3972 - val_loss: 377.1596 - val_accuracy: 0.2902\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 72s 406ms/step - loss: 2.2677 - accuracy: 0.4053 - val_loss: 412.6518 - val_accuracy: 0.2801\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 72s 402ms/step - loss: 2.2258 - accuracy: 0.4027 - val_loss: 398.9681 - val_accuracy: 0.2845\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 71s 400ms/step - loss: 2.1689 - accuracy: 0.4303 - val_loss: 421.6128 - val_accuracy: 0.2684\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.1416 - accuracy: 0.4334 - val_loss: 400.7722 - val_accuracy: 0.2938\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.1391 - accuracy: 0.4329 - val_loss: 409.9650 - val_accuracy: 0.2990\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.0973 - accuracy: 0.4417 - val_loss: 432.7876 - val_accuracy: 0.2916\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.0684 - accuracy: 0.4534 - val_loss: 426.4082 - val_accuracy: 0.2894\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 71s 398ms/step - loss: 2.0508 - accuracy: 0.4574 - val_loss: 383.3103 - val_accuracy: 0.3022\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.0311 - accuracy: 0.4711 - val_loss: 394.7541 - val_accuracy: 0.3096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a2dae700>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6542c",
   "metadata": {},
   "source": [
    "### simple 5 - further batch size increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5723bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e717ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74759b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 96)                1476192   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 1,481,232\n",
      "Trainable params: 1,481,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a806b1f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2a806b1f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "88/88 [==============================] - ETA: 0s - loss: 3.9098 - accuracy: 0.1059WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a807b160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2a807b160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "88/88 [==============================] - 71s 796ms/step - loss: 3.9062 - accuracy: 0.1063 - val_loss: 331.1077 - val_accuracy: 0.1795\n",
      "Epoch 2/20\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 3.0833 - accuracy: 0.2157 - val_loss: 338.9704 - val_accuracy: 0.2218\n",
      "Epoch 3/20\n",
      "88/88 [==============================] - 72s 816ms/step - loss: 2.8215 - accuracy: 0.2680 - val_loss: 369.7128 - val_accuracy: 0.2128\n",
      "Epoch 4/20\n",
      "88/88 [==============================] - 80s 905ms/step - loss: 2.6673 - accuracy: 0.3030 - val_loss: 385.1715 - val_accuracy: 0.2188\n",
      "Epoch 5/20\n",
      "88/88 [==============================] - 78s 871ms/step - loss: 2.5361 - accuracy: 0.3339 - val_loss: 381.9463 - val_accuracy: 0.2450\n",
      "Epoch 6/20\n",
      "88/88 [==============================] - 77s 866ms/step - loss: 2.4367 - accuracy: 0.3555 - val_loss: 400.7051 - val_accuracy: 0.2741\n",
      "Epoch 7/20\n",
      "88/88 [==============================] - 78s 885ms/step - loss: 2.3470 - accuracy: 0.3831 - val_loss: 422.3856 - val_accuracy: 0.2460\n",
      "Epoch 8/20\n",
      "88/88 [==============================] - 85s 958ms/step - loss: 2.3061 - accuracy: 0.3941 - val_loss: 441.3643 - val_accuracy: 0.2455\n",
      "Epoch 9/20\n",
      "88/88 [==============================] - 77s 867ms/step - loss: 2.2355 - accuracy: 0.4094 - val_loss: 410.5261 - val_accuracy: 0.2744\n",
      "Epoch 10/20\n",
      "88/88 [==============================] - 77s 872ms/step - loss: 2.2009 - accuracy: 0.4183 - val_loss: 409.0038 - val_accuracy: 0.2730\n",
      "Epoch 11/20\n",
      "88/88 [==============================] - 77s 867ms/step - loss: 2.1630 - accuracy: 0.4241 - val_loss: 463.6876 - val_accuracy: 0.2428\n",
      "Epoch 12/20\n",
      "88/88 [==============================] - 77s 871ms/step - loss: 2.1192 - accuracy: 0.4331 - val_loss: 436.9776 - val_accuracy: 0.2673\n",
      "Epoch 13/20\n",
      "88/88 [==============================] - 77s 868ms/step - loss: 2.0789 - accuracy: 0.4509 - val_loss: 471.1084 - val_accuracy: 0.2657\n",
      "Epoch 14/20\n",
      "88/88 [==============================] - 77s 865ms/step - loss: 2.0644 - accuracy: 0.4524 - val_loss: 477.9245 - val_accuracy: 0.2556\n",
      "Epoch 15/20\n",
      "88/88 [==============================] - 77s 864ms/step - loss: 2.0406 - accuracy: 0.4546 - val_loss: 462.8638 - val_accuracy: 0.2769\n",
      "Epoch 16/20\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 1.9598 - accuracy: 0.4666 - val_loss: 454.0520 - val_accuracy: 0.2668\n",
      "Epoch 17/20\n",
      "88/88 [==============================] - 77s 868ms/step - loss: 1.9521 - accuracy: 0.4747 - val_loss: 474.4875 - val_accuracy: 0.2692\n",
      "Epoch 18/20\n",
      "88/88 [==============================] - 75s 844ms/step - loss: 1.9417 - accuracy: 0.4714 - val_loss: 482.1344 - val_accuracy: 0.2635\n",
      "Epoch 19/20\n",
      "88/88 [==============================] - 76s 855ms/step - loss: 1.9169 - accuracy: 0.4824 - val_loss: 480.6531 - val_accuracy: 0.2717\n",
      "Epoch 20/20\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 1.8782 - accuracy: 0.4860 - val_loss: 506.7777 - val_accuracy: 0.2610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29ed14a90>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33164c2b",
   "metadata": {},
   "source": [
    "### simple 6 - revert to simple 4. add another dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c386f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b526a2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc2a66f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                984128    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 995,408\n",
      "Trainable params: 995,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa0bd940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2aa0bd940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 3.9529 - accuracy: 0.1022WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b56cb5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b56cb5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 76s 423ms/step - loss: 3.9512 - accuracy: 0.1024 - val_loss: 330.2222 - val_accuracy: 0.1658\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 74s 417ms/step - loss: 3.1556 - accuracy: 0.2035 - val_loss: 421.3106 - val_accuracy: 0.1710\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 75s 421ms/step - loss: 2.9556 - accuracy: 0.2547 - val_loss: 407.7954 - val_accuracy: 0.2076\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 76s 431ms/step - loss: 2.8133 - accuracy: 0.2697 - val_loss: 323.5945 - val_accuracy: 0.2662\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 76s 426ms/step - loss: 2.7035 - accuracy: 0.2970 - val_loss: 370.2302 - val_accuracy: 0.2392\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 75s 425ms/step - loss: 2.5880 - accuracy: 0.3310 - val_loss: 397.3971 - val_accuracy: 0.2387\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 75s 425ms/step - loss: 2.5582 - accuracy: 0.3357 - val_loss: 398.3760 - val_accuracy: 0.2608\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 74s 415ms/step - loss: 2.4338 - accuracy: 0.3594 - val_loss: 380.9827 - val_accuracy: 0.2957\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 72s 402ms/step - loss: 2.4527 - accuracy: 0.3637 - val_loss: 421.4738 - val_accuracy: 0.2520\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 2.3502 - accuracy: 0.3806 - val_loss: 438.5591 - val_accuracy: 0.2556\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 2.2165 - accuracy: 0.4093 - val_loss: 422.9698 - val_accuracy: 0.2496\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.2442 - accuracy: 0.4110 - val_loss: 479.7960 - val_accuracy: 0.2357\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 70s 391ms/step - loss: 2.1713 - accuracy: 0.4258 - val_loss: 382.9693 - val_accuracy: 0.3020\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.1132 - accuracy: 0.4390 - val_loss: 381.2980 - val_accuracy: 0.2981\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 71s 401ms/step - loss: 2.0676 - accuracy: 0.4466 - val_loss: 381.2368 - val_accuracy: 0.2968\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 73s 413ms/step - loss: 2.0178 - accuracy: 0.4621 - val_loss: 393.5655 - val_accuracy: 0.2979\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 71s 402ms/step - loss: 2.0201 - accuracy: 0.4577 - val_loss: 402.0599 - val_accuracy: 0.2796\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 71s 401ms/step - loss: 1.9696 - accuracy: 0.4709 - val_loss: 418.3690 - val_accuracy: 0.2780\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 71s 401ms/step - loss: 1.9625 - accuracy: 0.4884 - val_loss: 413.1035 - val_accuracy: 0.2870\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 1.9256 - accuracy: 0.4777 - val_loss: 395.7662 - val_accuracy: 0.3085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2aa0d7d90>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b62619",
   "metadata": {},
   "source": [
    "### simple 7 - adjust learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c794e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13d83a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e15cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b146c13e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                984128    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 995,408\n",
      "Trainable params: 995,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b77c20d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b77c20d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 3.9285 - accuracy: 0.1004WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b8137430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b8137430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 3.9267 - accuracy: 0.1007 - val_loss: 389.4145 - val_accuracy: 0.1538\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 3.1561 - accuracy: 0.2042 - val_loss: 433.8259 - val_accuracy: 0.1888\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.8819 - accuracy: 0.2617 - val_loss: 383.8553 - val_accuracy: 0.2264\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.7627 - accuracy: 0.2897 - val_loss: 423.3394 - val_accuracy: 0.2305\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 75s 425ms/step - loss: 2.6113 - accuracy: 0.3154 - val_loss: 402.9115 - val_accuracy: 0.2409\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 73s 413ms/step - loss: 2.5335 - accuracy: 0.3434 - val_loss: 385.8093 - val_accuracy: 0.2739\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 74s 414ms/step - loss: 2.4545 - accuracy: 0.3628 - val_loss: 439.0435 - val_accuracy: 0.2654\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.3957 - accuracy: 0.3813 - val_loss: 430.7149 - val_accuracy: 0.2627\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.3384 - accuracy: 0.3829 - val_loss: 430.7220 - val_accuracy: 0.2545\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.2708 - accuracy: 0.4016 - val_loss: 379.2365 - val_accuracy: 0.3085\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 71s 400ms/step - loss: 2.2146 - accuracy: 0.4135 - val_loss: 370.2410 - val_accuracy: 0.3142\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 71s 402ms/step - loss: 2.1454 - accuracy: 0.4315 - val_loss: 367.5629 - val_accuracy: 0.3148\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.0867 - accuracy: 0.4471 - val_loss: 376.0529 - val_accuracy: 0.3121\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 73s 412ms/step - loss: 2.1089 - accuracy: 0.4448 - val_loss: 376.7646 - val_accuracy: 0.3126\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 74s 416ms/step - loss: 2.0966 - accuracy: 0.4502 - val_loss: 379.2471 - val_accuracy: 0.3140\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 73s 412ms/step - loss: 2.0839 - accuracy: 0.4471 - val_loss: 382.7437 - val_accuracy: 0.3145\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 73s 411ms/step - loss: 2.0945 - accuracy: 0.4408 - val_loss: 383.3517 - val_accuracy: 0.3115\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0989 - accuracy: 0.4497 - val_loss: 386.4343 - val_accuracy: 0.3123\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.0720 - accuracy: 0.4547 - val_loss: 387.3797 - val_accuracy: 0.3118\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 72s 405ms/step - loss: 2.1047 - accuracy: 0.4400 - val_loss: 384.5558 - val_accuracy: 0.3145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b5db9d00>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac830ece",
   "metadata": {},
   "source": [
    "### simple 8 - improving generalization. add dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "487fba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb64432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13f4c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d04fd8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                984128    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 995,408\n",
      "Trainable params: 995,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b87c7700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b87c7700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 4.4508 - accuracy: 0.0710WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b8177160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b8177160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 71s 400ms/step - loss: 4.4480 - accuracy: 0.0712 - val_loss: 304.8165 - val_accuracy: 0.1462\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 72s 405ms/step - loss: 3.4110 - accuracy: 0.1561 - val_loss: 414.7056 - val_accuracy: 0.1517\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 71s 401ms/step - loss: 3.1813 - accuracy: 0.1942 - val_loss: 408.2963 - val_accuracy: 0.1686\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 3.0789 - accuracy: 0.2133 - val_loss: 440.6006 - val_accuracy: 0.1721\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 71s 398ms/step - loss: 2.9755 - accuracy: 0.2351 - val_loss: 460.9194 - val_accuracy: 0.1795\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 74s 418ms/step - loss: 2.9100 - accuracy: 0.2562 - val_loss: 476.2148 - val_accuracy: 0.1877\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 72s 407ms/step - loss: 2.7897 - accuracy: 0.2721 - val_loss: 411.6620 - val_accuracy: 0.2087\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 71s 402ms/step - loss: 2.7610 - accuracy: 0.2849 - val_loss: 450.6170 - val_accuracy: 0.2111\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 72s 406ms/step - loss: 2.6979 - accuracy: 0.2946 - val_loss: 456.6912 - val_accuracy: 0.2106\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.6189 - accuracy: 0.3128 - val_loss: 471.7690 - val_accuracy: 0.1937\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.5937 - accuracy: 0.3215 - val_loss: 436.2598 - val_accuracy: 0.2177\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.5442 - accuracy: 0.3363 - val_loss: 435.0240 - val_accuracy: 0.2248\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.5463 - accuracy: 0.3299 - val_loss: 434.9848 - val_accuracy: 0.2275\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 70s 397ms/step - loss: 2.5127 - accuracy: 0.3344 - val_loss: 435.9159 - val_accuracy: 0.2302\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 71s 402ms/step - loss: 2.5331 - accuracy: 0.3373 - val_loss: 441.6395 - val_accuracy: 0.2297\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 2.5010 - accuracy: 0.3465 - val_loss: 437.8658 - val_accuracy: 0.2316\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.4940 - accuracy: 0.3418 - val_loss: 441.3302 - val_accuracy: 0.2330\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.5180 - accuracy: 0.3406 - val_loss: 436.6660 - val_accuracy: 0.2343\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.4777 - accuracy: 0.3493 - val_loss: 442.1992 - val_accuracy: 0.2327\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.4992 - accuracy: 0.3456 - val_loss: 438.3827 - val_accuracy: 0.2357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b88e01f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023d2f5",
   "metadata": {},
   "source": [
    "### simple 9 - more generalization. increase dropout %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e00b6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf1eea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2a599ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3451baa8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                984128    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 995,408\n",
      "Trainable params: 995,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b88b1040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b88b1040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 3.9607 - accuracy: 0.0980WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b87d5550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b87d5550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 3.9591 - accuracy: 0.0982 - val_loss: 383.7683 - val_accuracy: 0.1716\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 3.2054 - accuracy: 0.2020 - val_loss: 361.7745 - val_accuracy: 0.1844\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 3.0301 - accuracy: 0.2185 - val_loss: 373.7593 - val_accuracy: 0.2141\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.8702 - accuracy: 0.2608 - val_loss: 376.2964 - val_accuracy: 0.2346\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.7332 - accuracy: 0.2810 - val_loss: 369.2934 - val_accuracy: 0.2280\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.6779 - accuracy: 0.2995 - val_loss: 407.7381 - val_accuracy: 0.2119\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.6114 - accuracy: 0.3184 - val_loss: 446.7303 - val_accuracy: 0.2179\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.5756 - accuracy: 0.3218 - val_loss: 381.8528 - val_accuracy: 0.2684\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.5148 - accuracy: 0.3400 - val_loss: 408.8759 - val_accuracy: 0.2575\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.4837 - accuracy: 0.3497 - val_loss: 419.8969 - val_accuracy: 0.2589\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.4042 - accuracy: 0.3679 - val_loss: 407.5877 - val_accuracy: 0.2700\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.3381 - accuracy: 0.3742 - val_loss: 400.4296 - val_accuracy: 0.2758\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.3332 - accuracy: 0.3839 - val_loss: 412.4786 - val_accuracy: 0.2711\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.2873 - accuracy: 0.3968 - val_loss: 406.8076 - val_accuracy: 0.2755\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.2713 - accuracy: 0.3917 - val_loss: 410.7323 - val_accuracy: 0.2733\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.2837 - accuracy: 0.3921 - val_loss: 414.2298 - val_accuracy: 0.2717\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.2922 - accuracy: 0.3998 - val_loss: 412.5172 - val_accuracy: 0.2690\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.2570 - accuracy: 0.4009 - val_loss: 415.8586 - val_accuracy: 0.2709\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.2872 - accuracy: 0.3942 - val_loss: 417.4745 - val_accuracy: 0.2711\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.2996 - accuracy: 0.3947 - val_loss: 421.0417 - val_accuracy: 0.2695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2aa9c5af0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropoutout(0.6))\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4ab7e",
   "metadata": {},
   "source": [
    "### simple 10. revert to simple 7 and apply normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95ba6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "69b5eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca2dc86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "153ea14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_26 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 31, 31, 16)        64        \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                984128    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 995,472\n",
      "Trainable params: 995,440\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2df7a93a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2df7a93a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 4.3484 - accuracy: 0.0906WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2df7e81f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2df7e81f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 70s 389ms/step - loss: 4.3460 - accuracy: 0.0907 - val_loss: 41.2292 - val_accuracy: 0.0652\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 3.4931 - accuracy: 0.1493 - val_loss: 96.8755 - val_accuracy: 0.0447\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 3.3802 - accuracy: 0.1576 - val_loss: 237.1734 - val_accuracy: 0.0911\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 3.2674 - accuracy: 0.1759 - val_loss: 316.3489 - val_accuracy: 0.0712\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 3.1517 - accuracy: 0.2069 - val_loss: 371.8242 - val_accuracy: 0.1072\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.9972 - accuracy: 0.2529 - val_loss: 469.1564 - val_accuracy: 0.1037\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.8868 - accuracy: 0.2606 - val_loss: 412.3338 - val_accuracy: 0.1233\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 69s 388ms/step - loss: 2.8172 - accuracy: 0.2806 - val_loss: 364.3688 - val_accuracy: 0.1394\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.7204 - accuracy: 0.2938 - val_loss: 379.0093 - val_accuracy: 0.1418\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 2.6644 - accuracy: 0.3036 - val_loss: 387.4370 - val_accuracy: 0.1440\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.4975 - accuracy: 0.3535 - val_loss: 388.5772 - val_accuracy: 0.1519\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 69s 388ms/step - loss: 2.4613 - accuracy: 0.3615 - val_loss: 387.0863 - val_accuracy: 0.1563\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.4796 - accuracy: 0.3558 - val_loss: 393.6887 - val_accuracy: 0.1568\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.4594 - accuracy: 0.3583 - val_loss: 390.3679 - val_accuracy: 0.1579\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.4378 - accuracy: 0.3657 - val_loss: 398.5926 - val_accuracy: 0.1541\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.4376 - accuracy: 0.3668 - val_loss: 399.8899 - val_accuracy: 0.1544\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.4476 - accuracy: 0.3574 - val_loss: 393.2794 - val_accuracy: 0.1596\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.4265 - accuracy: 0.3729 - val_loss: 394.9218 - val_accuracy: 0.1568\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.4246 - accuracy: 0.3673 - val_loss: 399.5331 - val_accuracy: 0.1549\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.4275 - accuracy: 0.3637 - val_loss: 402.9268 - val_accuracy: 0.1555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2df7e2250>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.BatchNormalizationtchNormalization())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abec48a",
   "metadata": {},
   "source": [
    "### simple 11. revert to simple 7. more epochs and smaller batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9af4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58d40ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49c10ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38d99d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                984128    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 995,408\n",
      "Trainable params: 995,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x34bfe9700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x34bfe9700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "354/354 [==============================] - ETA: 0s - loss: 3.8239 - accuracy: 0.1174WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x34bff5e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x34bff5e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "354/354 [==============================] - 69s 194ms/step - loss: 3.8231 - accuracy: 0.1176 - val_loss: 400.9291 - val_accuracy: 0.1405\n",
      "Epoch 2/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 3.1169 - accuracy: 0.2108 - val_loss: 356.0741 - val_accuracy: 0.1950\n",
      "Epoch 3/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.9036 - accuracy: 0.2572 - val_loss: 413.0000 - val_accuracy: 0.1860\n",
      "Epoch 4/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.7507 - accuracy: 0.2909 - val_loss: 367.9632 - val_accuracy: 0.2057\n",
      "Epoch 5/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.6236 - accuracy: 0.3195 - val_loss: 372.4343 - val_accuracy: 0.2482\n",
      "Epoch 6/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.5445 - accuracy: 0.3305 - val_loss: 401.6552 - val_accuracy: 0.1634\n",
      "Epoch 7/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.4640 - accuracy: 0.3504 - val_loss: 417.6642 - val_accuracy: 0.2163\n",
      "Epoch 8/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.3542 - accuracy: 0.3826 - val_loss: 435.2523 - val_accuracy: 0.2068\n",
      "Epoch 9/50\n",
      "354/354 [==============================] - 69s 195ms/step - loss: 2.3764 - accuracy: 0.3720 - val_loss: 354.0110 - val_accuracy: 0.2594\n",
      "Epoch 10/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.2593 - accuracy: 0.4031 - val_loss: 399.7370 - val_accuracy: 0.2278\n",
      "Epoch 11/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.1277 - accuracy: 0.4381 - val_loss: 367.5960 - val_accuracy: 0.2703\n",
      "Epoch 12/50\n",
      "354/354 [==============================] - 69s 195ms/step - loss: 2.1119 - accuracy: 0.4418 - val_loss: 367.1432 - val_accuracy: 0.2761\n",
      "Epoch 13/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.1034 - accuracy: 0.4423 - val_loss: 368.9110 - val_accuracy: 0.2807\n",
      "Epoch 14/50\n",
      "354/354 [==============================] - 69s 195ms/step - loss: 2.0930 - accuracy: 0.4480 - val_loss: 367.9052 - val_accuracy: 0.2807\n",
      "Epoch 15/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0756 - accuracy: 0.4525 - val_loss: 369.5785 - val_accuracy: 0.2815\n",
      "Epoch 16/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0903 - accuracy: 0.4513 - val_loss: 372.1874 - val_accuracy: 0.2823\n",
      "Epoch 17/50\n",
      "354/354 [==============================] - 69s 195ms/step - loss: 2.0820 - accuracy: 0.4482 - val_loss: 373.5416 - val_accuracy: 0.2804\n",
      "Epoch 18/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0504 - accuracy: 0.4521 - val_loss: 380.5611 - val_accuracy: 0.2807\n",
      "Epoch 19/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0661 - accuracy: 0.4550 - val_loss: 378.0575 - val_accuracy: 0.2853\n",
      "Epoch 20/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0528 - accuracy: 0.4567 - val_loss: 378.4841 - val_accuracy: 0.2834\n",
      "Epoch 21/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0671 - accuracy: 0.4539 - val_loss: 379.2930 - val_accuracy: 0.2807\n",
      "Epoch 22/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0553 - accuracy: 0.4546 - val_loss: 376.9083 - val_accuracy: 0.2842\n",
      "Epoch 23/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0397 - accuracy: 0.4591 - val_loss: 380.5477 - val_accuracy: 0.2840\n",
      "Epoch 24/50\n",
      "354/354 [==============================] - 69s 195ms/step - loss: 2.0493 - accuracy: 0.4534 - val_loss: 378.2253 - val_accuracy: 0.2853\n",
      "Epoch 25/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0595 - accuracy: 0.4572 - val_loss: 380.1581 - val_accuracy: 0.2834\n",
      "Epoch 26/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0527 - accuracy: 0.4529 - val_loss: 385.8637 - val_accuracy: 0.2812\n",
      "Epoch 27/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0608 - accuracy: 0.4529 - val_loss: 385.1058 - val_accuracy: 0.2818\n",
      "Epoch 28/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0319 - accuracy: 0.4577 - val_loss: 383.4231 - val_accuracy: 0.2823\n",
      "Epoch 29/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0508 - accuracy: 0.4574 - val_loss: 385.1536 - val_accuracy: 0.2829\n",
      "Epoch 30/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0402 - accuracy: 0.4620 - val_loss: 387.7769 - val_accuracy: 0.2837\n",
      "Epoch 31/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0090 - accuracy: 0.4637 - val_loss: 384.5868 - val_accuracy: 0.2837\n",
      "Epoch 32/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0159 - accuracy: 0.4700 - val_loss: 385.1366 - val_accuracy: 0.2831\n",
      "Epoch 33/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0273 - accuracy: 0.4569 - val_loss: 388.0470 - val_accuracy: 0.2821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0361 - accuracy: 0.4534 - val_loss: 385.2259 - val_accuracy: 0.2848\n",
      "Epoch 35/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0221 - accuracy: 0.4646 - val_loss: 385.4923 - val_accuracy: 0.2848\n",
      "Epoch 36/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0079 - accuracy: 0.4635 - val_loss: 390.3752 - val_accuracy: 0.2821\n",
      "Epoch 37/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0259 - accuracy: 0.4658 - val_loss: 387.6031 - val_accuracy: 0.2851\n",
      "Epoch 38/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0239 - accuracy: 0.4655 - val_loss: 388.3347 - val_accuracy: 0.2821\n",
      "Epoch 39/50\n",
      "354/354 [==============================] - 69s 195ms/step - loss: 2.0261 - accuracy: 0.4611 - val_loss: 387.2889 - val_accuracy: 0.2845\n",
      "Epoch 40/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 1.9984 - accuracy: 0.4620 - val_loss: 384.7624 - val_accuracy: 0.2861\n",
      "Epoch 41/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0518 - accuracy: 0.4547 - val_loss: 390.7475 - val_accuracy: 0.2815\n",
      "Epoch 42/50\n",
      "354/354 [==============================] - 69s 195ms/step - loss: 2.0174 - accuracy: 0.4694 - val_loss: 391.7320 - val_accuracy: 0.2810\n",
      "Epoch 43/50\n",
      "354/354 [==============================] - 70s 196ms/step - loss: 2.0269 - accuracy: 0.4589 - val_loss: 390.6549 - val_accuracy: 0.2861\n",
      "Epoch 44/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0062 - accuracy: 0.4674 - val_loss: 392.3664 - val_accuracy: 0.2812\n",
      "Epoch 45/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0184 - accuracy: 0.4628 - val_loss: 393.0568 - val_accuracy: 0.2840\n",
      "Epoch 46/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0105 - accuracy: 0.4646 - val_loss: 394.0614 - val_accuracy: 0.2842\n",
      "Epoch 47/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0362 - accuracy: 0.4546 - val_loss: 388.5776 - val_accuracy: 0.2856\n",
      "Epoch 48/50\n",
      "354/354 [==============================] - 69s 196ms/step - loss: 2.0129 - accuracy: 0.4644 - val_loss: 395.8984 - val_accuracy: 0.2837\n",
      "Epoch 49/50\n",
      "354/354 [==============================] - 69s 195ms/step - loss: 2.0062 - accuracy: 0.4707 - val_loss: 390.3846 - val_accuracy: 0.2870\n",
      "Epoch 50/50\n",
      "354/354 [==============================] - 70s 197ms/step - loss: 2.0096 - accuracy: 0.4636 - val_loss: 388.2249 - val_accuracy: 0.2883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3186ab8b0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=50,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de841f1",
   "metadata": {},
   "source": [
    "### simple 12. revert to simple 7. use sgd instead of adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "850de1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d0ca9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "21982ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ff95923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 64)                984128    \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 995,408\n",
      "Trainable params: 995,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2df7e8c10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2df7e8c10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 4.2789 - accuracy: 0.0618WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x34c034ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x34c034ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 4.2783 - accuracy: 0.0618 - val_loss: 218.8149 - val_accuracy: 0.1203\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 71s 398ms/step - loss: 3.9541 - accuracy: 0.0888 - val_loss: 287.2503 - val_accuracy: 0.1298\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 72s 404ms/step - loss: 3.7128 - accuracy: 0.1267 - val_loss: 285.7245 - val_accuracy: 0.1397\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 3.5367 - accuracy: 0.1522 - val_loss: 354.5566 - val_accuracy: 0.1331\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 3.3987 - accuracy: 0.1697 - val_loss: 375.5279 - val_accuracy: 0.1563\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 3.3032 - accuracy: 0.1834 - val_loss: 382.7085 - val_accuracy: 0.1585\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 72s 407ms/step - loss: 3.2104 - accuracy: 0.1971 - val_loss: 525.4379 - val_accuracy: 0.1211\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 73s 409ms/step - loss: 3.1677 - accuracy: 0.2033 - val_loss: 381.4981 - val_accuracy: 0.1836\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 73s 408ms/step - loss: 3.0911 - accuracy: 0.2187 - val_loss: 385.9117 - val_accuracy: 0.1792\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 73s 413ms/step - loss: 3.0673 - accuracy: 0.2251 - val_loss: 456.2794 - val_accuracy: 0.1555\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 73s 411ms/step - loss: 3.0250 - accuracy: 0.2275 - val_loss: 441.8534 - val_accuracy: 0.1645\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 72s 406ms/step - loss: 3.0010 - accuracy: 0.2309 - val_loss: 430.4474 - val_accuracy: 0.1688\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 71s 401ms/step - loss: 2.9957 - accuracy: 0.2399 - val_loss: 420.7853 - val_accuracy: 0.1762\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 71s 402ms/step - loss: 3.0054 - accuracy: 0.2321 - val_loss: 412.7984 - val_accuracy: 0.1787\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 71s 402ms/step - loss: 2.9700 - accuracy: 0.2390 - val_loss: 406.2870 - val_accuracy: 0.1819\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 71s 401ms/step - loss: 2.9801 - accuracy: 0.2466 - val_loss: 401.0378 - val_accuracy: 0.1844\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 2.9708 - accuracy: 0.2498 - val_loss: 396.5884 - val_accuracy: 0.1877\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 71s 398ms/step - loss: 2.9880 - accuracy: 0.2479 - val_loss: 392.7169 - val_accuracy: 0.1896\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 2.9669 - accuracy: 0.2492 - val_loss: 389.3887 - val_accuracy: 0.1926\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 72s 403ms/step - loss: 2.9789 - accuracy: 0.2440 - val_loss: 386.4619 - val_accuracy: 0.1959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a071c490>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09158de4",
   "metadata": {},
   "source": [
    "### simple 13. revert to simple 7. still need to reduce overfitting. attempt different data augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a00cf3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "91dce729",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6eee3db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=60,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.4,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a282d36a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_32 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                984128    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 995,408\n",
      "Trainable params: 995,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x3567b09d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x3567b09d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 4.0233 - accuracy: 0.0864WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x38560f700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x38560f700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 73s 409ms/step - loss: 4.0216 - accuracy: 0.0866 - val_loss: 320.6267 - val_accuracy: 0.1579\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 73s 409ms/step - loss: 3.2891 - accuracy: 0.1769 - val_loss: 309.4710 - val_accuracy: 0.1969\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 72s 407ms/step - loss: 3.0300 - accuracy: 0.2260 - val_loss: 266.4610 - val_accuracy: 0.2253\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 73s 410ms/step - loss: 2.8959 - accuracy: 0.2596 - val_loss: 260.2416 - val_accuracy: 0.2316\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 73s 409ms/step - loss: 2.7892 - accuracy: 0.2798 - val_loss: 307.3312 - val_accuracy: 0.2005\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 72s 408ms/step - loss: 2.7037 - accuracy: 0.3011 - val_loss: 282.5999 - val_accuracy: 0.2357\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 71s 398ms/step - loss: 2.6394 - accuracy: 0.3186 - val_loss: 271.7997 - val_accuracy: 0.2387\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 2.6311 - accuracy: 0.3254 - val_loss: 268.9425 - val_accuracy: 0.2624\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 72s 403ms/step - loss: 2.5586 - accuracy: 0.3359 - val_loss: 257.3876 - val_accuracy: 0.2580\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 71s 400ms/step - loss: 2.5090 - accuracy: 0.3500 - val_loss: 302.3326 - val_accuracy: 0.2703\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 71s 400ms/step - loss: 2.4111 - accuracy: 0.3618 - val_loss: 289.0665 - val_accuracy: 0.2807\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 71s 401ms/step - loss: 2.3704 - accuracy: 0.3749 - val_loss: 284.1434 - val_accuracy: 0.2867\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 70s 397ms/step - loss: 2.3580 - accuracy: 0.3788 - val_loss: 283.3693 - val_accuracy: 0.2897\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 71s 398ms/step - loss: 2.3435 - accuracy: 0.3935 - val_loss: 281.6228 - val_accuracy: 0.2954\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 2.3364 - accuracy: 0.3899 - val_loss: 282.0843 - val_accuracy: 0.2954\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 71s 398ms/step - loss: 2.3283 - accuracy: 0.3978 - val_loss: 280.9585 - val_accuracy: 0.2962\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 71s 400ms/step - loss: 2.3157 - accuracy: 0.3882 - val_loss: 281.0449 - val_accuracy: 0.2979\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.3034 - accuracy: 0.3925 - val_loss: 281.1461 - val_accuracy: 0.2954\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.3255 - accuracy: 0.3890 - val_loss: 281.5977 - val_accuracy: 0.2998\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.3156 - accuracy: 0.3978 - val_loss: 283.2121 - val_accuracy: 0.2995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3561c8940>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c0b53",
   "metadata": {},
   "source": [
    "### simple 14. revert to simple 7. adding more layers. sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9cf476af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c132770",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2187887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44afa55f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_34 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 28, 29, 8)         1544      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 7, 9, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 504)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 64)                32320     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 45,144\n",
      "Trainable params: 45,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x38563e940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x38563e940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 4.5419 - accuracy: 0.0354WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x389b95f70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x389b95f70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 73s 407ms/step - loss: 4.5416 - accuracy: 0.0355 - val_loss: 284.6983 - val_accuracy: 0.0567\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 71s 398ms/step - loss: 4.2281 - accuracy: 0.0605 - val_loss: 188.6545 - val_accuracy: 0.0633\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 71s 400ms/step - loss: 4.1111 - accuracy: 0.0603 - val_loss: 140.3525 - val_accuracy: 0.0622\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 73s 408ms/step - loss: 3.9896 - accuracy: 0.0843 - val_loss: 184.2632 - val_accuracy: 0.0636\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 73s 410ms/step - loss: 3.8415 - accuracy: 0.1062 - val_loss: 231.2752 - val_accuracy: 0.0783\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 72s 405ms/step - loss: 3.7429 - accuracy: 0.1115 - val_loss: 267.6020 - val_accuracy: 0.0867\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 71s 401ms/step - loss: 3.6557 - accuracy: 0.1269 - val_loss: 304.5971 - val_accuracy: 0.0876\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 3.5625 - accuracy: 0.1330 - val_loss: 384.0466 - val_accuracy: 0.0693\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 3.5195 - accuracy: 0.1415 - val_loss: 389.9878 - val_accuracy: 0.0679\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 3.4804 - accuracy: 0.1481 - val_loss: 390.9165 - val_accuracy: 0.0859\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 72s 404ms/step - loss: 3.4251 - accuracy: 0.1515 - val_loss: 391.8581 - val_accuracy: 0.0857\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 72s 407ms/step - loss: 3.4349 - accuracy: 0.1546 - val_loss: 392.3624 - val_accuracy: 0.0857\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 71s 401ms/step - loss: 3.4319 - accuracy: 0.1481 - val_loss: 392.5785 - val_accuracy: 0.0865\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 3.4453 - accuracy: 0.1551 - val_loss: 392.3657 - val_accuracy: 0.0854\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 3.4442 - accuracy: 0.1478 - val_loss: 392.0609 - val_accuracy: 0.0851\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 3.4312 - accuracy: 0.1504 - val_loss: 391.5685 - val_accuracy: 0.0846\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 3.4053 - accuracy: 0.1515 - val_loss: 390.9456 - val_accuracy: 0.0843\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 3.4620 - accuracy: 0.1491 - val_loss: 390.6623 - val_accuracy: 0.0859\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 3.4144 - accuracy: 0.1562 - val_loss: 390.1759 - val_accuracy: 0.0857\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 3.4408 - accuracy: 0.1565 - val_loss: 389.7266 - val_accuracy: 0.0851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x389bc5310>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Conv2D(8, (4,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((4,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c1072",
   "metadata": {},
   "source": [
    "### simple 15. use simple 14 but with adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cddaf547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f6721b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f41cb3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "824eed87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_37 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 28, 29, 8)         1544      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 7, 9, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 504)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                32320     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 45,144\n",
      "Trainable params: 45,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x37ce7fc10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x37ce7fc10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 4.1258 - accuracy: 0.0715WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x498d8c670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x498d8c670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 4.1242 - accuracy: 0.0717 - val_loss: 308.7679 - val_accuracy: 0.1489\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 3.4032 - accuracy: 0.1515 - val_loss: 315.5456 - val_accuracy: 0.1716\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 3.1217 - accuracy: 0.2077 - val_loss: 330.8274 - val_accuracy: 0.1639\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.9984 - accuracy: 0.2299 - val_loss: 350.7546 - val_accuracy: 0.1762\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.9145 - accuracy: 0.2424 - val_loss: 362.3940 - val_accuracy: 0.1989\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.8870 - accuracy: 0.2492 - val_loss: 320.8654 - val_accuracy: 0.1929\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.8324 - accuracy: 0.2705 - val_loss: 326.0032 - val_accuracy: 0.2343\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.7795 - accuracy: 0.2672 - val_loss: 344.0689 - val_accuracy: 0.2196\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.7393 - accuracy: 0.2790 - val_loss: 321.3187 - val_accuracy: 0.2411\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.6836 - accuracy: 0.2886 - val_loss: 339.0219 - val_accuracy: 0.2583\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.6019 - accuracy: 0.3061 - val_loss: 321.2126 - val_accuracy: 0.2632\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.5785 - accuracy: 0.3227 - val_loss: 317.0931 - val_accuracy: 0.2662\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.5755 - accuracy: 0.3186 - val_loss: 317.1425 - val_accuracy: 0.2657\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.5708 - accuracy: 0.3311 - val_loss: 316.1453 - val_accuracy: 0.2657\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.5304 - accuracy: 0.3282 - val_loss: 314.9957 - val_accuracy: 0.2673\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.5487 - accuracy: 0.3297 - val_loss: 317.3191 - val_accuracy: 0.2700\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.5538 - accuracy: 0.3333 - val_loss: 315.3411 - val_accuracy: 0.2700\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.5677 - accuracy: 0.3283 - val_loss: 318.0095 - val_accuracy: 0.2676\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.5221 - accuracy: 0.3372 - val_loss: 317.8761 - val_accuracy: 0.2690\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.5296 - accuracy: 0.3306 - val_loss: 317.0912 - val_accuracy: 0.2711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x37cf21ee0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Conv2D(8, (4,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((4,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ec735",
   "metadata": {},
   "source": [
    "### exact copy of simple 7. running for reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6a1287ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "309f49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "325273f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "42d5a738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_40 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 64)                984128    \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 995,408\n",
      "Trainable params: 995,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x498d7ff70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x498d7ff70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 4.2053 - accuracy: 0.0892WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x50ffd50d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x50ffd50d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 4.2030 - accuracy: 0.0894 - val_loss: 354.5447 - val_accuracy: 0.1585\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 3.2322 - accuracy: 0.1881 - val_loss: 344.2645 - val_accuracy: 0.2163\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.9449 - accuracy: 0.2421 - val_loss: 480.8427 - val_accuracy: 0.1830\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.8301 - accuracy: 0.2739 - val_loss: 347.2731 - val_accuracy: 0.2750\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.6806 - accuracy: 0.3047 - val_loss: 386.2916 - val_accuracy: 0.2480\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 2.5761 - accuracy: 0.3352 - val_loss: 394.6487 - val_accuracy: 0.2690\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.5622 - accuracy: 0.3298 - val_loss: 438.1072 - val_accuracy: 0.2504\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.4476 - accuracy: 0.3554 - val_loss: 437.1156 - val_accuracy: 0.2646\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.3869 - accuracy: 0.3714 - val_loss: 463.8456 - val_accuracy: 0.2578\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 70s 391ms/step - loss: 2.3473 - accuracy: 0.3820 - val_loss: 428.1234 - val_accuracy: 0.2575\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 2.2087 - accuracy: 0.4122 - val_loss: 416.0967 - val_accuracy: 0.2785\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.1664 - accuracy: 0.4327 - val_loss: 416.8193 - val_accuracy: 0.2834\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 69s 392ms/step - loss: 2.1376 - accuracy: 0.4311 - val_loss: 419.6717 - val_accuracy: 0.2823\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 2.1330 - accuracy: 0.4433 - val_loss: 424.8620 - val_accuracy: 0.2837\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.1279 - accuracy: 0.4451 - val_loss: 426.2368 - val_accuracy: 0.2831\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.1163 - accuracy: 0.4473 - val_loss: 429.8698 - val_accuracy: 0.2831\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.1114 - accuracy: 0.4472 - val_loss: 430.0681 - val_accuracy: 0.2848\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.1262 - accuracy: 0.4390 - val_loss: 432.8949 - val_accuracy: 0.2848\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 2.0932 - accuracy: 0.4547 - val_loss: 438.7909 - val_accuracy: 0.2845\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.1253 - accuracy: 0.4357 - val_loss: 441.5883 - val_accuracy: 0.2831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x498fd1070>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f51af8",
   "metadata": {},
   "source": [
    "### adding more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8c3ebeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "66e1a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b9fe6727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11331 images belonging to 96 classes.\n",
      "Found 3666 images belonging to 96 classes.\n",
      "Found 1271 images belonging to 96 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fba9cb1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_42 (Conv2D)           (None, 192, 192, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 94, 94, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 64)                984128    \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 96)                6240      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 995,408\n",
      "Trainable params: 995,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x51cb950d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x51cb950d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - ETA: 0s - loss: 3.8500 - accuracy: 0.1191WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x59c65b820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x59c65b820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "177/177 [==============================] - 70s 390ms/step - loss: 3.8485 - accuracy: 0.1193 - val_loss: 291.8517 - val_accuracy: 0.1882\n",
      "Epoch 2/100\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 3.1755 - accuracy: 0.1990 - val_loss: 368.9975 - val_accuracy: 0.2264\n",
      "Epoch 3/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.9170 - accuracy: 0.2475 - val_loss: 374.8955 - val_accuracy: 0.2313\n",
      "Epoch 4/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.7711 - accuracy: 0.2865 - val_loss: 420.3694 - val_accuracy: 0.2174\n",
      "Epoch 5/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.6975 - accuracy: 0.2988 - val_loss: 571.3348 - val_accuracy: 0.1866\n",
      "Epoch 6/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.5643 - accuracy: 0.3374 - val_loss: 395.3942 - val_accuracy: 0.2548\n",
      "Epoch 7/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.4863 - accuracy: 0.3493 - val_loss: 447.9033 - val_accuracy: 0.2526\n",
      "Epoch 8/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.4049 - accuracy: 0.3698 - val_loss: 448.8775 - val_accuracy: 0.2406\n",
      "Epoch 9/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.3687 - accuracy: 0.3772 - val_loss: 493.9675 - val_accuracy: 0.2190\n",
      "Epoch 10/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.3069 - accuracy: 0.3867 - val_loss: 643.0703 - val_accuracy: 0.2059\n",
      "Epoch 11/100\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.2118 - accuracy: 0.4127 - val_loss: 550.9780 - val_accuracy: 0.2335\n",
      "Epoch 12/100\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 2.1862 - accuracy: 0.4187 - val_loss: 535.4053 - val_accuracy: 0.2409\n",
      "Epoch 13/100\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 2.1735 - accuracy: 0.4278 - val_loss: 528.5253 - val_accuracy: 0.2436\n",
      "Epoch 14/100\n",
      "177/177 [==============================] - 76s 425ms/step - loss: 2.1271 - accuracy: 0.4360 - val_loss: 530.9865 - val_accuracy: 0.2433\n",
      "Epoch 15/100\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.1547 - accuracy: 0.4309 - val_loss: 529.9174 - val_accuracy: 0.2439\n",
      "Epoch 16/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.1389 - accuracy: 0.4323 - val_loss: 533.5838 - val_accuracy: 0.2441\n",
      "Epoch 17/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.1022 - accuracy: 0.4387 - val_loss: 536.3359 - val_accuracy: 0.2420\n",
      "Epoch 18/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.1468 - accuracy: 0.4369 - val_loss: 537.9016 - val_accuracy: 0.2414\n",
      "Epoch 19/100\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.1536 - accuracy: 0.4329 - val_loss: 537.9677 - val_accuracy: 0.2417\n",
      "Epoch 20/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.1021 - accuracy: 0.4456 - val_loss: 534.0432 - val_accuracy: 0.2436\n",
      "Epoch 21/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.1059 - accuracy: 0.4426 - val_loss: 542.3792 - val_accuracy: 0.2387\n",
      "Epoch 22/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0925 - accuracy: 0.4433 - val_loss: 541.1140 - val_accuracy: 0.2395\n",
      "Epoch 23/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0992 - accuracy: 0.4467 - val_loss: 548.5654 - val_accuracy: 0.2381\n",
      "Epoch 24/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0923 - accuracy: 0.4536 - val_loss: 548.1212 - val_accuracy: 0.2362\n",
      "Epoch 25/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0889 - accuracy: 0.4483 - val_loss: 541.6057 - val_accuracy: 0.2403\n",
      "Epoch 26/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0929 - accuracy: 0.4477 - val_loss: 551.5037 - val_accuracy: 0.2354\n",
      "Epoch 27/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.1054 - accuracy: 0.4415 - val_loss: 551.9158 - val_accuracy: 0.2370\n",
      "Epoch 28/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0951 - accuracy: 0.4559 - val_loss: 562.7655 - val_accuracy: 0.2330\n",
      "Epoch 29/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0852 - accuracy: 0.4539 - val_loss: 558.0524 - val_accuracy: 0.2354\n",
      "Epoch 30/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0849 - accuracy: 0.4426 - val_loss: 549.5828 - val_accuracy: 0.2398\n",
      "Epoch 31/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0715 - accuracy: 0.4571 - val_loss: 554.2473 - val_accuracy: 0.2368\n",
      "Epoch 32/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0616 - accuracy: 0.4522 - val_loss: 559.0737 - val_accuracy: 0.2370\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 70s 391ms/step - loss: 2.0686 - accuracy: 0.4510 - val_loss: 562.7220 - val_accuracy: 0.2362\n",
      "Epoch 34/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0957 - accuracy: 0.4497 - val_loss: 556.9500 - val_accuracy: 0.2373\n",
      "Epoch 35/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0822 - accuracy: 0.4512 - val_loss: 564.5050 - val_accuracy: 0.2340\n",
      "Epoch 36/100\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 2.0842 - accuracy: 0.4446 - val_loss: 556.7069 - val_accuracy: 0.2365\n",
      "Epoch 37/100\n",
      "177/177 [==============================] - 72s 404ms/step - loss: 2.0856 - accuracy: 0.4424 - val_loss: 571.2230 - val_accuracy: 0.2327\n",
      "Epoch 38/100\n",
      "177/177 [==============================] - 71s 401ms/step - loss: 2.0573 - accuracy: 0.4525 - val_loss: 553.0322 - val_accuracy: 0.2390\n",
      "Epoch 39/100\n",
      "177/177 [==============================] - 72s 404ms/step - loss: 2.0654 - accuracy: 0.4579 - val_loss: 559.6155 - val_accuracy: 0.2387\n",
      "Epoch 40/100\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 2.1003 - accuracy: 0.4528 - val_loss: 568.2133 - val_accuracy: 0.2351\n",
      "Epoch 41/100\n",
      "177/177 [==============================] - 71s 399ms/step - loss: 2.0744 - accuracy: 0.4500 - val_loss: 556.5797 - val_accuracy: 0.2395\n",
      "Epoch 42/100\n",
      "177/177 [==============================] - 70s 395ms/step - loss: 2.0523 - accuracy: 0.4562 - val_loss: 565.1183 - val_accuracy: 0.2357\n",
      "Epoch 43/100\n",
      "177/177 [==============================] - 73s 408ms/step - loss: 2.0468 - accuracy: 0.4560 - val_loss: 567.5991 - val_accuracy: 0.2360\n",
      "Epoch 44/100\n",
      "177/177 [==============================] - 72s 405ms/step - loss: 2.0516 - accuracy: 0.4520 - val_loss: 565.5287 - val_accuracy: 0.2362\n",
      "Epoch 45/100\n",
      "177/177 [==============================] - 70s 396ms/step - loss: 2.0452 - accuracy: 0.4579 - val_loss: 568.1147 - val_accuracy: 0.2349\n",
      "Epoch 46/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0629 - accuracy: 0.4536 - val_loss: 565.3945 - val_accuracy: 0.2360\n",
      "Epoch 47/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0737 - accuracy: 0.4511 - val_loss: 571.7599 - val_accuracy: 0.2357\n",
      "Epoch 48/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0657 - accuracy: 0.4519 - val_loss: 561.9846 - val_accuracy: 0.2360\n",
      "Epoch 49/100\n",
      "177/177 [==============================] - 70s 391ms/step - loss: 2.1102 - accuracy: 0.4470 - val_loss: 566.1554 - val_accuracy: 0.2376\n",
      "Epoch 50/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0778 - accuracy: 0.4471 - val_loss: 569.6808 - val_accuracy: 0.2351\n",
      "Epoch 51/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0425 - accuracy: 0.4583 - val_loss: 567.3556 - val_accuracy: 0.2357\n",
      "Epoch 52/100\n",
      "177/177 [==============================] - 70s 391ms/step - loss: 2.0646 - accuracy: 0.4533 - val_loss: 570.3049 - val_accuracy: 0.2351\n",
      "Epoch 53/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0404 - accuracy: 0.4584 - val_loss: 559.6078 - val_accuracy: 0.2381\n",
      "Epoch 54/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0570 - accuracy: 0.4643 - val_loss: 564.9313 - val_accuracy: 0.2370\n",
      "Epoch 55/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0491 - accuracy: 0.4531 - val_loss: 572.5813 - val_accuracy: 0.2362\n",
      "Epoch 56/100\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 2.0566 - accuracy: 0.4555 - val_loss: 575.0842 - val_accuracy: 0.2362\n",
      "Epoch 57/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0372 - accuracy: 0.4573 - val_loss: 574.2852 - val_accuracy: 0.2346\n",
      "Epoch 58/100\n",
      "177/177 [==============================] - 70s 391ms/step - loss: 2.0404 - accuracy: 0.4559 - val_loss: 579.4521 - val_accuracy: 0.2338\n",
      "Epoch 59/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0285 - accuracy: 0.4670 - val_loss: 570.8779 - val_accuracy: 0.2346\n",
      "Epoch 60/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0418 - accuracy: 0.4549 - val_loss: 570.1730 - val_accuracy: 0.2349\n",
      "Epoch 61/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0530 - accuracy: 0.4605 - val_loss: 565.3649 - val_accuracy: 0.2351\n",
      "Epoch 62/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0787 - accuracy: 0.4534 - val_loss: 568.6298 - val_accuracy: 0.2357\n",
      "Epoch 63/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0428 - accuracy: 0.4650 - val_loss: 569.5182 - val_accuracy: 0.2340\n",
      "Epoch 64/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0566 - accuracy: 0.4542 - val_loss: 574.7576 - val_accuracy: 0.2321\n",
      "Epoch 65/100\n",
      "177/177 [==============================] - 70s 394ms/step - loss: 2.0197 - accuracy: 0.4656 - val_loss: 571.1311 - val_accuracy: 0.2332\n",
      "Epoch 66/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0414 - accuracy: 0.4590 - val_loss: 575.9645 - val_accuracy: 0.2327\n",
      "Epoch 67/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0459 - accuracy: 0.4541 - val_loss: 580.6335 - val_accuracy: 0.2313\n",
      "Epoch 68/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0237 - accuracy: 0.4648 - val_loss: 575.8074 - val_accuracy: 0.2332\n",
      "Epoch 69/100\n",
      "177/177 [==============================] - 70s 393ms/step - loss: 2.0686 - accuracy: 0.4609 - val_loss: 581.7025 - val_accuracy: 0.2302\n",
      "Epoch 70/100\n",
      "177/177 [==============================] - 70s 392ms/step - loss: 2.0301 - accuracy: 0.4634 - val_loss: 580.2046 - val_accuracy: 0.2319\n",
      "Epoch 71/100\n",
      "177/177 [==============================] - 70s 391ms/step - loss: 2.0152 - accuracy: 0.4647 - val_loss: 573.6460 - val_accuracy: 0.2349\n",
      "Epoch 72/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0384 - accuracy: 0.4626 - val_loss: 572.1282 - val_accuracy: 0.2338\n",
      "Epoch 73/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0556 - accuracy: 0.4608 - val_loss: 568.9052 - val_accuracy: 0.2340\n",
      "Epoch 74/100\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 2.0235 - accuracy: 0.4640 - val_loss: 578.4131 - val_accuracy: 0.2338\n",
      "Epoch 75/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0409 - accuracy: 0.4601 - val_loss: 588.1589 - val_accuracy: 0.2327\n",
      "Epoch 76/100\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 2.0183 - accuracy: 0.4614 - val_loss: 574.1691 - val_accuracy: 0.2335\n",
      "Epoch 77/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0607 - accuracy: 0.4600 - val_loss: 584.7371 - val_accuracy: 0.2327\n",
      "Epoch 78/100\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.0551 - accuracy: 0.4528 - val_loss: 587.2656 - val_accuracy: 0.2310\n",
      "Epoch 79/100\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.0392 - accuracy: 0.4658 - val_loss: 583.7085 - val_accuracy: 0.2324\n",
      "Epoch 80/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0446 - accuracy: 0.4559 - val_loss: 579.3425 - val_accuracy: 0.2340\n",
      "Epoch 81/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0017 - accuracy: 0.4733 - val_loss: 581.8414 - val_accuracy: 0.2330\n",
      "Epoch 82/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0482 - accuracy: 0.4554 - val_loss: 588.1069 - val_accuracy: 0.2335\n",
      "Epoch 83/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0335 - accuracy: 0.4662 - val_loss: 579.9081 - val_accuracy: 0.2357\n",
      "Epoch 84/100\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.0528 - accuracy: 0.4630 - val_loss: 587.6719 - val_accuracy: 0.2324\n",
      "Epoch 85/100\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.0548 - accuracy: 0.4559 - val_loss: 591.4878 - val_accuracy: 0.2302\n",
      "Epoch 86/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0147 - accuracy: 0.4628 - val_loss: 589.8355 - val_accuracy: 0.2291\n",
      "Epoch 87/100\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.0556 - accuracy: 0.4593 - val_loss: 591.4723 - val_accuracy: 0.2316\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 69s 389ms/step - loss: 2.0222 - accuracy: 0.4709 - val_loss: 589.0053 - val_accuracy: 0.2300\n",
      "Epoch 89/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0017 - accuracy: 0.4658 - val_loss: 597.3110 - val_accuracy: 0.2267\n",
      "Epoch 90/100\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.0177 - accuracy: 0.4596 - val_loss: 600.6490 - val_accuracy: 0.2272\n",
      "Epoch 91/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0494 - accuracy: 0.4597 - val_loss: 594.1898 - val_accuracy: 0.2291\n",
      "Epoch 92/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0359 - accuracy: 0.4642 - val_loss: 598.7492 - val_accuracy: 0.2259\n",
      "Epoch 93/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0092 - accuracy: 0.4637 - val_loss: 597.6760 - val_accuracy: 0.2280\n",
      "Epoch 94/100\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.0134 - accuracy: 0.4615 - val_loss: 599.6483 - val_accuracy: 0.2289\n",
      "Epoch 95/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0337 - accuracy: 0.4670 - val_loss: 598.8539 - val_accuracy: 0.2270\n",
      "Epoch 96/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0216 - accuracy: 0.4678 - val_loss: 599.9413 - val_accuracy: 0.2280\n",
      "Epoch 97/100\n",
      "177/177 [==============================] - 69s 390ms/step - loss: 2.0283 - accuracy: 0.4602 - val_loss: 604.3193 - val_accuracy: 0.2264\n",
      "Epoch 98/100\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 1.9922 - accuracy: 0.4715 - val_loss: 593.5669 - val_accuracy: 0.2297\n",
      "Epoch 99/100\n",
      "177/177 [==============================] - 69s 389ms/step - loss: 2.0384 - accuracy: 0.4642 - val_loss: 594.9199 - val_accuracy: 0.2291\n",
      "Epoch 100/100\n",
      "177/177 [==============================] - 69s 391ms/step - loss: 2.0322 - accuracy: 0.4691 - val_loss: 597.5081 - val_accuracy: 0.2275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x51cbbf3a0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=11331 // BATCH_SIZE, epochs=100,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160dc81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

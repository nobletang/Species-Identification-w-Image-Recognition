{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ac549f",
   "metadata": {},
   "source": [
    "![title](assets/flower_banner.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a05d22",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c64f6",
   "metadata": {},
   "source": [
    "In this journal, a machine learning model is used to identify flowers through image classification. This multi-classification model is trained with thousands of images which are randomly augmented to achieve better generalization. The model serves to be a proof of concept in battling invasive plant species that damage the fragile ecosystem in which they reside. This journal will contain the steps taken to arrive at a similar model. Analysis of the results will be mentioned in the later sections, followed by what the results mean for the stakeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e394a",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a2890",
   "metadata": {},
   "source": [
    "Every year, the U.S. is estimated to lose $120B due to the impact of invasive plant species. Invasive plant species can reduce yield in nearby agriculture, kill existing plants, increase the risk of forest fires, and much more. While the spread of invasive plant species can be slowed through customs, the plants that reside in America right now need to be found and removed. Programs to remove these plants are already in place, but can be accelerated through the use of machine learning and scouting tools like drones. Drones can quickly scout dangerous terrain and machine learning can quickly review photos to identify an invasive plant species. This proposal will help reduce cost spent in manual labor and remove the risk of workers traversing dangerous terrain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ba2b2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "#from tensorflow.keras.applications import Xception, DenseNet201\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "#import random\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137b02c",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image size options: 192\n",
    "IMAGE_DIMENSION = 192\n",
    "VECTOR_LEN = IMAGE_DIMENSION**2\n",
    "NUM_CLASS = 96\n",
    "\n",
    "train_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/train'\n",
    "val_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/val'\n",
    "test_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/test'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_BATCH_SIZE = BATCH_SIZE\n",
    "VAL_BATCH_SIZE = BATCH_SIZE\n",
    "TEST_BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1015a998",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec9cbe0",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ba8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subfolder_list(source_folder):\n",
    "    # returns a list of all the subfolders in the source_folder\n",
    "    class_list_dir = []\n",
    "\n",
    "    for file in os.listdir(source_folder):\n",
    "        d = os.path.join(source_folder, file)\n",
    "        if os.path.isdir(d):\n",
    "            class_list_dir.append(d)\n",
    "\n",
    "    return class_list_dir\n",
    "\n",
    "def map_classes(subfolder_list):\n",
    "    # returns a dict with the flower as the key, and (count, directory) as the values\n",
    "    class_dict = {}\n",
    "\n",
    "    for class_folder in subfolder_list:\n",
    "        file_count = sum(len(files) for _, _, files in os.walk(class_folder))\n",
    "        class_dict[class_folder[24:]] = file_count, class_folder\n",
    "        \n",
    "    return class_dict\n",
    "\n",
    "def get_flower_count(class_dict):\n",
    "    # returns a list of the number of flowers found in the dict\n",
    "    flower_count = []\n",
    "\n",
    "    for flower in list(class_dict.values()):\n",
    "        flower_count.append(flower[0])\n",
    "\n",
    "    return flower_count\n",
    "\n",
    "def get_metrics(flower_count):\n",
    "    # returns basic metrics when given a list of flower value count\n",
    "    class_std = np.std(flower_count)\n",
    "    class_max = max(flower_count)\n",
    "    class_min = min(flower_count)\n",
    "    class_mean = np.mean(flower_count)\n",
    "    class_first_quartile = np.percentile(flower_count, 25)\n",
    "    class_third_quartile = np.percentile(flower_count, 75)\n",
    "    class_tenth_percentile = np.percentile(flower_count, 10)\n",
    "    class_fifth_percentile = np.percentile(flower_count, 5)\n",
    "\n",
    "    print(f'0. standard deviation: {class_std}')\n",
    "    print(f'1. max: {class_max}')\n",
    "    print(f'2. min: {class_min}')\n",
    "    print(f'3. mean: {class_mean}')\n",
    "    print(f'4. 25%: {class_first_quartile}')\n",
    "    print(f'5. 75%: {class_third_quartile}')\n",
    "    print(f'6. 10%: {class_tenth_percentile}')\n",
    "    print(f'7. 5%: {class_fifth_percentile}')\n",
    "    \n",
    "    return (class_std, class_max, class_min, class_mean, class_first_quartile,\n",
    "class_third_quartile, class_tenth_percentile, class_fifth_percentile)\n",
    "\n",
    "def plot_distribution(keys, values):\n",
    "    list1 = list(train_dict.keys())\n",
    "    list2 = train_flower_count\n",
    "    \n",
    "    dict_list = {}\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        dict_list[list1[i]] = list2[i]\n",
    "\n",
    "    dict_list\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(dict_list, orient='index')\n",
    "\n",
    "    df_sorted = df.sort_values(0, ascending=False)\n",
    "\n",
    "    flower_name = list(df_sorted.index)\n",
    "    value_count = list(df_sorted[0])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 20))\n",
    "\n",
    "    ax.barh(flower_name, value_count);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677be926",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subfolders = get_subfolder_list(train_dir)\n",
    "train_dict = map_classes(train_subfolders)\n",
    "train_flower_count = get_flower_count(train_dict)\n",
    "\n",
    "train_subfolders[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(train_flower_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = list(train_dict.keys())\n",
    "list2 = train_flower_count\n",
    "\n",
    "plot_distribution(list1, list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d41c79",
   "metadata": {},
   "source": [
    "There are 104 different flower species provided in this dataset, 16,463 images with different compositions. An image can be of a distant rose bush, a macro photo of a sunflower bud, or a portrait with a hibiscus tucked behind the ear. The distribution of these flowers is not equal and would result in some preprocessing before models should be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beccaf5e",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55501e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_subfolder(source):\n",
    "    # copies source folder to a new directory with '_new' attached to the end of the folder name\n",
    "    prev_dir_index = source.rfind('/')\n",
    "    destination = source[:prev_dir_index] + '_new' + source[prev_dir_index:]\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(destination):\n",
    "        result = shutil.copytree(source, destination, symlinks=False, ignore=None, \n",
    "                                 copy_function=shutil.copy2, ignore_dangling_symlinks=False, \n",
    "                                 dirs_exist_ok=False)\n",
    "    else:\n",
    "        print(f'{destination} already exists')\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def item_count(folder):\n",
    "    # helper function to identify which folders to remove\n",
    "    file_count = sum(len(files) for _, _, files in os.walk(folder))\n",
    "    \n",
    "    return file_count\n",
    "    \n",
    "def get_short_list(subfolder_list, n):\n",
    "    # find a list of classes that are divided by the specified n value\n",
    "    temp_dict = map_classes(subfolder_list)\n",
    "    temp_flower_count = get_flower_count(temp_dict)\n",
    "    \n",
    "    move_list = []\n",
    "    ignore_list = []\n",
    "    \n",
    "    percent_cutoff = np.percentile(temp_flower_count, n)\n",
    "    \n",
    "    remove_count = 0\n",
    "    \n",
    "    for subfolder in subfolder_list:\n",
    "        if item_count(subfolder) >= percent_cutoff:\n",
    "            move_list.append(subfolder)\n",
    "        else:\n",
    "            ignore_list.append(subfolder)\n",
    "            remove_count += item_count(subfolder)\n",
    "            \n",
    "    print(f'removed {remove_count} images')\n",
    "    \n",
    "    return move_list, ignore_list\n",
    "    \n",
    "def trim(subfolder_list):\n",
    "    # main function used to copy classes into new train, val, test \n",
    "    for subfolder in subfolder_list:\n",
    "        copy_subfolder(subfolder)\n",
    "\n",
    "        val_subfolder = subfolder.replace('/train/', '/val/')\n",
    "        copy_subfolder(val_subfolder)\n",
    "\n",
    "        test_subfolder = subfolder.replace('/train/', '/test/')\n",
    "        copy_subfolder(test_subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2c4bc",
   "metadata": {},
   "source": [
    "The structure of the directory has class labels for the train and validation folder, but the test folder contains images without any labels. To create a typical train, validate, and test structure, the images in the test folder will be ignored from this point on. 10% of the images found in the train folder will be moved into a new test folder, resulting in three folders with labeled images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_list, ignore_list = get_short_list(train_subfolders, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needs to be run once if the direectory is not yet set up\n",
    "trim(move_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the directories as new folders are made \n",
    "train_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/train_new'\n",
    "val_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/val_new'\n",
    "test_dir = f'data/jpeg-{IMAGE_DIMENSION}x{IMAGE_DIMENSION}/test_new'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def1770",
   "metadata": {},
   "source": [
    "With this directory set up, classes of flowers that do not have enough information on which to train a model can be removed. Functions have been created to select these folders to avoid manually separating the folders by hand. In the models shown in the notebook, the 10th percentile of the number of images found in the training are removed from the scope. This leaves 96 classes with 16,268 images. On average, there should be 168 images per class that can be divided into train, validation, and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53f713",
   "metadata": {},
   "source": [
    "With nearly 100 different classes, there are not enough images to effectively train a model. Data augmentation will be used to make the model more generalizable. Each image will randomly flip, change in brightness, crop, and many other transformations. Also, with these images being RGB, the images will need to be standardized from 0 and 255 to the range of 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     horizontal_flip=True, \n",
    "                                     rotation_range=45,\n",
    "                                     vertical_flip=False,\n",
    "                                     brightness_range=[0.75,1.25],\n",
    "                                     zoom_range=0.2,\n",
    "                                     shear_range=0.2\n",
    "                                    ).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION), \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "val_generator = ImageDataGenerator().flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe5142",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05788959",
   "metadata": {},
   "source": [
    "Through many iterations, the final model comes out to be a convolutional neural network (CNN) with several layers. The model looks through each image and run for 20 epochs. There is also a learning rate that reduces the rate at which the model trains. The goal of the learning rate is to prevent training loss to get ahead of the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba607370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051cdcc7",
   "metadata": {},
   "source": [
    "There were some attempts at bettering the model during training but metrics were not improved. The final model faced much overfitting issues but did not use BatchNormalization nor Dropout layers. Adding these layers did not help improve the validation accuracy metric. When attempting to introduce transfer learning, the results instead reduced the validation accuracy metric and increased validation loss. With shorter run times and better metrics, these layers were removed from the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f20dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, \n",
    "                        kernel_size=(2,2),\n",
    "                        activation='relu',\n",
    "                        padding = 'same',\n",
    "                        input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
    "                        data_format = 'channels_last'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3,3)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(NUM_CLASS)) # output layer\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training begins\n",
    "model.fit(train_generator, steps_per_epoch=len(train_generator)*10 // BATCH_SIZE, epochs=20,\n",
    "          validation_data=val_generator, callbacks=lr_callback, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b654a76",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06f701",
   "metadata": {},
   "source": [
    "With the model fully trained, the best accuracy of the model was 31% validation accuracy. The same model is evaluated against the test set and performed at 28% test accuracy. The result is a model that can make classifications better than random guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(test_generator, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83acee7",
   "metadata": {},
   "source": [
    "The model could be improved by having more images of flowers to train on. More images means the model could identify just the important features. Also, perhaps the model could improve if more resources were available, like RAM or GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a0590",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc0aad",
   "metadata": {},
   "source": [
    "Image classification of flowers is possible with what was gathered for the model. 31% of the images looked at by this model are correctly identified. While the model could be improved, this is a proof of concept for the U.S. Department of Agriculture. The model has much room for improvement given more time and resources, but for the scope of the project, a model has been trained on images to identify the species of a flower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
